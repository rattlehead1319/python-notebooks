{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Face_mask_segmentation_Questions_Project_CV_AIML_Online.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VvWl3ebqzCc1"},"source":["# Instructions\n","- Some parts of the code are already done for you\n","- You need to execute all the cells\n","- You need to add the code where ever you see `\"#### Add your code here ####\"`\n","- Marks are mentioned along with the cells"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NgR0j5310qqC"},"source":["# Face Mask Segmentation\n","Task is to predict the boundaries(mask) around the face in a given image."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Aa0jyJzw091I"},"source":["## Dataset\n","Faces in images marked with bounding boxes. Have around 500 images with around 1100 faces manually tagged via bounding box."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CjRTlPkp1LC2"},"source":["### Mount Google drive if you are using google colab\n","- We recommend using Google Colab as you can face memory issues and longer runtimes while running on local"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sBWMoTJ9cf3Z","outputId":"f4b7f64d-c935-4820-ad66-fa79cc69d64a","executionInfo":{"status":"ok","timestamp":1589947959147,"user_tz":-330,"elapsed":33243,"user":{"displayName":"Atul Kumar","photoUrl":"","userId":"02267315536838712153"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sO9mgMmp13sI"},"source":["### Change current working directory to project folder (1 mark)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TddMnf4D1-59","colab":{}},"source":["import os\n","os.chdir('/content/drive/My Drive/Colab Notebooks/Computer Vision/')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3srplE-FEpKa"},"source":["### Load the \"images.npy\" file (2 marks)\n","- This file contains images with details of bounding boxes"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MqFE_tZDf0sM","colab":{}},"source":["import numpy as np\n","data = np.load('images.npy', allow_pickle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_SMP8zliFT7R"},"source":["### Check one sample from the loaded \"images.npy\" file  (2 marks)\n","Hint - print data[10][1] "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NoqNvPK-iXqG","outputId":"bf91373a-1154-4e90-d5cf-aa38b6cb9840","executionInfo":{"status":"ok","timestamp":1589947988958,"user_tz":-330,"elapsed":1388,"user":{"displayName":"Atul Kumar","photoUrl":"","userId":"02267315536838712153"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(data[5][1])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[{'label': ['Face'], 'notes': '', 'points': [{'x': 0.486, 'y': 0.046}, {'x': 0.678, 'y': 0.272}], 'imageWidth': 500, 'imageHeight': 500}, {'label': ['Face'], 'notes': '', 'points': [{'x': 0.374, 'y': 0.524}, {'x': 0.542, 'y': 0.794}], 'imageWidth': 500, 'imageHeight': 500}]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m94G4p3CE5Cj"},"source":["### Set image dimensions   (2 marks)\n","- Initialize image height, image width with value: 224 "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kuZmtOASevDo","colab":{}},"source":["IMAGE_WIDTH = 224\n","IMAGE_HEIGHT = 224"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wY6FEsCjG47s"},"source":["### Create features and labels\n","- Here feature is the image\n","- The label is the mask\n","- Images will be stored in \"X_train\" array\n","- Masks will be stored in \"masks\" array"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XjCT9EVTgAvr","colab":{}},"source":["import cv2\n","from tensorflow.keras.applications.mobilenet import preprocess_input\n","\n","masks = np.zeros((int(data.shape[0]), IMAGE_HEIGHT, IMAGE_WIDTH))\n","X_train = np.zeros((int(data.shape[0]), IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n","for index in range(data.shape[0]):\n","    img = data[index][0]\n","    img = cv2.resize(img, dsize=(IMAGE_HEIGHT, IMAGE_WIDTH), interpolation=cv2.INTER_CUBIC)\n","    try:\n","      img = img[:, :, :3]\n","    except:\n","      continue\n","    X_train[index] = preprocess_input(np.array(img, dtype=np.float32))\n","    for i in data[index][1]:\n","        x1 = int(i[\"points\"][0]['x'] * IMAGE_WIDTH)\n","        x2 = int(i[\"points\"][1]['x'] * IMAGE_WIDTH)\n","        y1 = int(i[\"points\"][0]['y'] * IMAGE_HEIGHT)\n","        y2 = int(i[\"points\"][1]['y'] * IMAGE_HEIGHT)\n","        masks[index][y1:y2, x1:x2] = 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"N3AYbP79bFtJ"},"source":["### Print the shape of X_train and mask array  (2 marks)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3PIRaEdWIjDa","outputId":"d25dc600-c2fd-4034-9659-0a179d0786bd","executionInfo":{"status":"ok","timestamp":1589948004201,"user_tz":-330,"elapsed":1893,"user":{"displayName":"Atul Kumar","photoUrl":"","userId":"02267315536838712153"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(X_train.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["(409, 224, 224, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Gw6uH5DxgI_r","outputId":"50599300-684c-40dd-d40e-034d7a8cc6ff","executionInfo":{"status":"ok","timestamp":1589948009321,"user_tz":-330,"elapsed":2315,"user":{"displayName":"Atul Kumar","photoUrl":"","userId":"02267315536838712153"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(masks.shape)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["(409, 224, 224)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"R4wgkWq1bk5F"},"source":["### Print a sample image and image array"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qfRZjQufj0N9","outputId":"1c31e65c-bb46-488e-c743-0def69b12aa5","executionInfo":{"status":"ok","timestamp":1589948015379,"user_tz":-330,"elapsed":3316,"user":{"displayName":"Atul Kumar","photoUrl":"","userId":"02267315536838712153"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from matplotlib import pyplot\n","n = 10\n","print(X_train[n])\n","pyplot.imshow(X_train[n])"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"stream","text":["[[[-0.98431373 -0.98431373 -0.98431373]\n","  [-0.98431373 -0.98431373 -0.98431373]\n","  [-0.98431373 -0.98431373 -0.98431373]\n","  ...\n","  [-1.         -1.         -1.        ]\n","  [-1.         -1.         -1.        ]\n","  [-1.         -1.         -1.        ]]\n","\n"," [[-0.98431373 -0.98431373 -0.98431373]\n","  [-0.98431373 -0.98431373 -0.98431373]\n","  [-0.98431373 -0.98431373 -0.98431373]\n","  ...\n","  [-1.         -1.         -1.        ]\n","  [-1.         -1.         -1.        ]\n","  [-1.         -1.         -1.        ]]\n","\n"," [[-0.98431373 -0.98431373 -0.98431373]\n","  [-0.98431373 -0.98431373 -0.98431373]\n","  [-0.98431373 -0.98431373 -0.98431373]\n","  ...\n","  [-1.         -1.         -1.        ]\n","  [-1.         -1.         -1.        ]\n","  [-1.         -1.         -1.        ]]\n","\n"," ...\n","\n"," [[-1.         -1.         -1.        ]\n","  [-1.         -1.         -1.        ]\n","  [-1.         -1.         -1.        ]\n","  ...\n","  [-0.96862745 -0.96862745 -0.96862745]\n","  [-0.96078432 -0.96078432 -0.96078432]\n","  [-0.96078432 -0.96078432 -0.96078432]]\n","\n"," [[-1.         -1.         -1.        ]\n","  [-1.         -1.         -1.        ]\n","  [-1.         -1.         -1.        ]\n","  ...\n","  [-0.96862745 -0.96862745 -0.96862745]\n","  [-0.96078432 -0.96078432 -0.96078432]\n","  [-0.95294118 -0.95294118 -0.95294118]]\n","\n"," [[-1.         -1.         -1.        ]\n","  [-1.         -1.         -1.        ]\n","  [-1.         -1.         -1.        ]\n","  ...\n","  [-0.97647059 -0.97647059 -0.97647059]\n","  [-0.96862745 -0.96862745 -0.96862745]\n","  [-0.96078432 -0.96078432 -0.96078432]]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fdc16201400>"]},"metadata":{"tags":[]},"execution_count":11},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"983gGKcwlSe8","outputId":"7ff834fa-6d76-4ec4-dd68-22042108e2b0","executionInfo":{"status":"ok","timestamp":1589948022960,"user_tz":-330,"elapsed":3097,"user":{"displayName":"Atul Kumar","photoUrl":"","userId":"02267315536838712153"}},"colab":{"base_uri":"https://localhost:8080/","height":286}},"source":["pyplot.imshow(masks[n])"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fdc20070390>"]},"metadata":{"tags":[]},"execution_count":12},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Z0Qa1UOue9TE"},"source":["## Create the model (10 marks)\n","- Add MobileNet as model with below parameter values\n","  - input_shape: IMAGE_HEIGHT, IMAGE_WIDTH, 3\n","  - include_top: False\n","  - alpha: 1.0\n","  - weights: \"imagenet\"\n","- Add UNET architecture layers\n","  - This is the trickiest part of the project, you need to research and implement it correctly"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BTVYOvANrUVx","colab":{}},"source":["from tensorflow.keras.applications.mobilenet import MobileNet\n","from tensorflow.keras.layers import Concatenate, UpSampling2D, Conv2D, Reshape\n","from tensorflow.keras.models import Model\n","\n","\n","def create_model(trainable=True):\n","    model = MobileNet(input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3), alpha=1.0, include_top=False, weights= \"imagenet\")\n","    for layer in model.layers:\n","        layer.trainable = trainable\n","\n","    # Add all the UNET layers here\n","    \n","    block1 = model._layers[0].output\n","    block2 = model.get_layer(\"conv_pw_1_relu\").output\n","    block3 = model.get_layer(\"conv_pw_2_relu\").output\n","    block4 = model.get_layer(\"conv_pw_5_relu\").output\n","    block5 = model.get_layer(\"conv_pw_11_relu\").output\n","    block6 = model.get_layer(\"conv_pw_13_relu\").output\n","\n","    x = Concatenate()([UpSampling2D()(block6), block5])\n","    x = Concatenate()([UpSampling2D()(x), block4])\n","    x = Concatenate()([UpSampling2D()(x), block3])\n","    x = Concatenate()([UpSampling2D()(x), block2])\n","    x = Concatenate()([UpSampling2D()(x), block1])\n","\n","    x = Conv2D(1, kernel_size=1, activation=\"sigmoid\")(x)\n","    x = Reshape((IMAGE_HEIGHT, IMAGE_WIDTH))(x)\n","\n","    return Model(inputs=model.inputs, outputs=x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_snZ9o0ZBAiv"},"source":["### Call the create_model function"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9TfSSP51uPoO","outputId":"62a7d517-defd-4084-8c19-3d94995ad4ac","executionInfo":{"status":"ok","timestamp":1589950135677,"user_tz":-330,"elapsed":2384,"user":{"displayName":"Atul Kumar","photoUrl":"","userId":"02267315536838712153"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Give trainable=False as argument, if you want to freeze lower layers for fast training (but low accuracy)\n","model = create_model(trainable=False)\n","\n","# Print summary\n","model.summary()"],"execution_count":51,"outputs":[{"output_type":"stream","text":["Model: \"model_4\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_5 (InputLayer)            [(None, 224, 224, 3) 0                                            \n","__________________________________________________________________________________________________\n","conv1_pad (ZeroPadding2D)       (None, 225, 225, 3)  0           input_5[0][0]                    \n","__________________________________________________________________________________________________\n","conv1 (Conv2D)                  (None, 112, 112, 32) 864         conv1_pad[0][0]                  \n","__________________________________________________________________________________________________\n","conv1_bn (BatchNormalization)   (None, 112, 112, 32) 128         conv1[0][0]                      \n","__________________________________________________________________________________________________\n","conv1_relu (ReLU)               (None, 112, 112, 32) 0           conv1_bn[0][0]                   \n","__________________________________________________________________________________________________\n","conv_dw_1 (DepthwiseConv2D)     (None, 112, 112, 32) 288         conv1_relu[0][0]                 \n","__________________________________________________________________________________________________\n","conv_dw_1_bn (BatchNormalizatio (None, 112, 112, 32) 128         conv_dw_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv_dw_1_relu (ReLU)           (None, 112, 112, 32) 0           conv_dw_1_bn[0][0]               \n","__________________________________________________________________________________________________\n","conv_pw_1 (Conv2D)              (None, 112, 112, 64) 2048        conv_dw_1_relu[0][0]             \n","__________________________________________________________________________________________________\n","conv_pw_1_bn (BatchNormalizatio (None, 112, 112, 64) 256         conv_pw_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv_pw_1_relu (ReLU)           (None, 112, 112, 64) 0           conv_pw_1_bn[0][0]               \n","__________________________________________________________________________________________________\n","conv_pad_2 (ZeroPadding2D)      (None, 113, 113, 64) 0           conv_pw_1_relu[0][0]             \n","__________________________________________________________________________________________________\n","conv_dw_2 (DepthwiseConv2D)     (None, 56, 56, 64)   576         conv_pad_2[0][0]                 \n","__________________________________________________________________________________________________\n","conv_dw_2_bn (BatchNormalizatio (None, 56, 56, 64)   256         conv_dw_2[0][0]                  \n","__________________________________________________________________________________________________\n","conv_dw_2_relu (ReLU)           (None, 56, 56, 64)   0           conv_dw_2_bn[0][0]               \n","__________________________________________________________________________________________________\n","conv_pw_2 (Conv2D)              (None, 56, 56, 128)  8192        conv_dw_2_relu[0][0]             \n","__________________________________________________________________________________________________\n","conv_pw_2_bn (BatchNormalizatio (None, 56, 56, 128)  512         conv_pw_2[0][0]                  \n","__________________________________________________________________________________________________\n","conv_pw_2_relu (ReLU)           (None, 56, 56, 128)  0           conv_pw_2_bn[0][0]               \n","__________________________________________________________________________________________________\n","conv_dw_3 (DepthwiseConv2D)     (None, 56, 56, 128)  1152        conv_pw_2_relu[0][0]             \n","__________________________________________________________________________________________________\n","conv_dw_3_bn (BatchNormalizatio (None, 56, 56, 128)  512         conv_dw_3[0][0]                  \n","__________________________________________________________________________________________________\n","conv_dw_3_relu (ReLU)           (None, 56, 56, 128)  0           conv_dw_3_bn[0][0]               \n","__________________________________________________________________________________________________\n","conv_pw_3 (Conv2D)              (None, 56, 56, 128)  16384       conv_dw_3_relu[0][0]             \n","__________________________________________________________________________________________________\n","conv_pw_3_bn (BatchNormalizatio (None, 56, 56, 128)  512         conv_pw_3[0][0]                  \n","__________________________________________________________________________________________________\n","conv_pw_3_relu (ReLU)           (None, 56, 56, 128)  0           conv_pw_3_bn[0][0]               \n","__________________________________________________________________________________________________\n","conv_pad_4 (ZeroPadding2D)      (None, 57, 57, 128)  0           conv_pw_3_relu[0][0]             \n","__________________________________________________________________________________________________\n","conv_dw_4 (DepthwiseConv2D)     (None, 28, 28, 128)  1152        conv_pad_4[0][0]                 \n","__________________________________________________________________________________________________\n","conv_dw_4_bn (BatchNormalizatio (None, 28, 28, 128)  512         conv_dw_4[0][0]                  \n","__________________________________________________________________________________________________\n","conv_dw_4_relu (ReLU)           (None, 28, 28, 128)  0           conv_dw_4_bn[0][0]               \n","__________________________________________________________________________________________________\n","conv_pw_4 (Conv2D)              (None, 28, 28, 256)  32768       conv_dw_4_relu[0][0]             \n","__________________________________________________________________________________________________\n","conv_pw_4_bn (BatchNormalizatio (None, 28, 28, 256)  1024        conv_pw_4[0][0]                  \n","__________________________________________________________________________________________________\n","conv_pw_4_relu (ReLU)           (None, 28, 28, 256)  0           conv_pw_4_bn[0][0]               \n","__________________________________________________________________________________________________\n","conv_dw_5 (DepthwiseConv2D)     (None, 28, 28, 256)  2304        conv_pw_4_relu[0][0]             \n","__________________________________________________________________________________________________\n","conv_dw_5_bn (BatchNormalizatio (None, 28, 28, 256)  1024        conv_dw_5[0][0]                  \n","__________________________________________________________________________________________________\n","conv_dw_5_relu (ReLU)           (None, 28, 28, 256)  0           conv_dw_5_bn[0][0]               \n","__________________________________________________________________________________________________\n","conv_pw_5 (Conv2D)              (None, 28, 28, 256)  65536       conv_dw_5_relu[0][0]             \n","__________________________________________________________________________________________________\n","conv_pw_5_bn (BatchNormalizatio (None, 28, 28, 256)  1024        conv_pw_5[0][0]                  \n","__________________________________________________________________________________________________\n","conv_pw_5_relu (ReLU)           (None, 28, 28, 256)  0           conv_pw_5_bn[0][0]               \n","__________________________________________________________________________________________________\n","conv_pad_6 (ZeroPadding2D)      (None, 29, 29, 256)  0           conv_pw_5_relu[0][0]             \n","__________________________________________________________________________________________________\n","conv_dw_6 (DepthwiseConv2D)     (None, 14, 14, 256)  2304        conv_pad_6[0][0]                 \n","__________________________________________________________________________________________________\n","conv_dw_6_bn (BatchNormalizatio (None, 14, 14, 256)  1024        conv_dw_6[0][0]                  \n","__________________________________________________________________________________________________\n","conv_dw_6_relu (ReLU)           (None, 14, 14, 256)  0           conv_dw_6_bn[0][0]               \n","__________________________________________________________________________________________________\n","conv_pw_6 (Conv2D)              (None, 14, 14, 512)  131072      conv_dw_6_relu[0][0]             \n","__________________________________________________________________________________________________\n","conv_pw_6_bn (BatchNormalizatio (None, 14, 14, 512)  2048        conv_pw_6[0][0]                  \n","__________________________________________________________________________________________________\n","conv_pw_6_relu (ReLU)           (None, 14, 14, 512)  0           conv_pw_6_bn[0][0]               \n","__________________________________________________________________________________________________\n","conv_dw_7 (DepthwiseConv2D)     (None, 14, 14, 512)  4608        conv_pw_6_relu[0][0]             \n","__________________________________________________________________________________________________\n","conv_dw_7_bn (BatchNormalizatio (None, 14, 14, 512)  2048        conv_dw_7[0][0]                  \n","__________________________________________________________________________________________________\n","conv_dw_7_relu (ReLU)           (None, 14, 14, 512)  0           conv_dw_7_bn[0][0]               \n","__________________________________________________________________________________________________\n","conv_pw_7 (Conv2D)              (None, 14, 14, 512)  262144      conv_dw_7_relu[0][0]             \n","__________________________________________________________________________________________________\n","conv_pw_7_bn (BatchNormalizatio (None, 14, 14, 512)  2048        conv_pw_7[0][0]                  \n","__________________________________________________________________________________________________\n","conv_pw_7_relu (ReLU)           (None, 14, 14, 512)  0           conv_pw_7_bn[0][0]               \n","__________________________________________________________________________________________________\n","conv_dw_8 (DepthwiseConv2D)     (None, 14, 14, 512)  4608        conv_pw_7_relu[0][0]             \n","__________________________________________________________________________________________________\n","conv_dw_8_bn (BatchNormalizatio (None, 14, 14, 512)  2048        conv_dw_8[0][0]                  \n","__________________________________________________________________________________________________\n","conv_dw_8_relu (ReLU)           (None, 14, 14, 512)  0           conv_dw_8_bn[0][0]               \n","__________________________________________________________________________________________________\n","conv_pw_8 (Conv2D)              (None, 14, 14, 512)  262144      conv_dw_8_relu[0][0]             \n","__________________________________________________________________________________________________\n","conv_pw_8_bn (BatchNormalizatio (None, 14, 14, 512)  2048        conv_pw_8[0][0]                  \n","__________________________________________________________________________________________________\n","conv_pw_8_relu (ReLU)           (None, 14, 14, 512)  0           conv_pw_8_bn[0][0]               \n","__________________________________________________________________________________________________\n","conv_dw_9 (DepthwiseConv2D)     (None, 14, 14, 512)  4608        conv_pw_8_relu[0][0]             \n","__________________________________________________________________________________________________\n","conv_dw_9_bn (BatchNormalizatio (None, 14, 14, 512)  2048        conv_dw_9[0][0]                  \n","__________________________________________________________________________________________________\n","conv_dw_9_relu (ReLU)           (None, 14, 14, 512)  0           conv_dw_9_bn[0][0]               \n","__________________________________________________________________________________________________\n","conv_pw_9 (Conv2D)              (None, 14, 14, 512)  262144      conv_dw_9_relu[0][0]             \n","__________________________________________________________________________________________________\n","conv_pw_9_bn (BatchNormalizatio (None, 14, 14, 512)  2048        conv_pw_9[0][0]                  \n","__________________________________________________________________________________________________\n","conv_pw_9_relu (ReLU)           (None, 14, 14, 512)  0           conv_pw_9_bn[0][0]               \n","__________________________________________________________________________________________________\n","conv_dw_10 (DepthwiseConv2D)    (None, 14, 14, 512)  4608        conv_pw_9_relu[0][0]             \n","__________________________________________________________________________________________________\n","conv_dw_10_bn (BatchNormalizati (None, 14, 14, 512)  2048        conv_dw_10[0][0]                 \n","__________________________________________________________________________________________________\n","conv_dw_10_relu (ReLU)          (None, 14, 14, 512)  0           conv_dw_10_bn[0][0]              \n","__________________________________________________________________________________________________\n","conv_pw_10 (Conv2D)             (None, 14, 14, 512)  262144      conv_dw_10_relu[0][0]            \n","__________________________________________________________________________________________________\n","conv_pw_10_bn (BatchNormalizati (None, 14, 14, 512)  2048        conv_pw_10[0][0]                 \n","__________________________________________________________________________________________________\n","conv_pw_10_relu (ReLU)          (None, 14, 14, 512)  0           conv_pw_10_bn[0][0]              \n","__________________________________________________________________________________________________\n","conv_dw_11 (DepthwiseConv2D)    (None, 14, 14, 512)  4608        conv_pw_10_relu[0][0]            \n","__________________________________________________________________________________________________\n","conv_dw_11_bn (BatchNormalizati (None, 14, 14, 512)  2048        conv_dw_11[0][0]                 \n","__________________________________________________________________________________________________\n","conv_dw_11_relu (ReLU)          (None, 14, 14, 512)  0           conv_dw_11_bn[0][0]              \n","__________________________________________________________________________________________________\n","conv_pw_11 (Conv2D)             (None, 14, 14, 512)  262144      conv_dw_11_relu[0][0]            \n","__________________________________________________________________________________________________\n","conv_pw_11_bn (BatchNormalizati (None, 14, 14, 512)  2048        conv_pw_11[0][0]                 \n","__________________________________________________________________________________________________\n","conv_pw_11_relu (ReLU)          (None, 14, 14, 512)  0           conv_pw_11_bn[0][0]              \n","__________________________________________________________________________________________________\n","conv_pad_12 (ZeroPadding2D)     (None, 15, 15, 512)  0           conv_pw_11_relu[0][0]            \n","__________________________________________________________________________________________________\n","conv_dw_12 (DepthwiseConv2D)    (None, 7, 7, 512)    4608        conv_pad_12[0][0]                \n","__________________________________________________________________________________________________\n","conv_dw_12_bn (BatchNormalizati (None, 7, 7, 512)    2048        conv_dw_12[0][0]                 \n","__________________________________________________________________________________________________\n","conv_dw_12_relu (ReLU)          (None, 7, 7, 512)    0           conv_dw_12_bn[0][0]              \n","__________________________________________________________________________________________________\n","conv_pw_12 (Conv2D)             (None, 7, 7, 1024)   524288      conv_dw_12_relu[0][0]            \n","__________________________________________________________________________________________________\n","conv_pw_12_bn (BatchNormalizati (None, 7, 7, 1024)   4096        conv_pw_12[0][0]                 \n","__________________________________________________________________________________________________\n","conv_pw_12_relu (ReLU)          (None, 7, 7, 1024)   0           conv_pw_12_bn[0][0]              \n","__________________________________________________________________________________________________\n","conv_dw_13 (DepthwiseConv2D)    (None, 7, 7, 1024)   9216        conv_pw_12_relu[0][0]            \n","__________________________________________________________________________________________________\n","conv_dw_13_bn (BatchNormalizati (None, 7, 7, 1024)   4096        conv_dw_13[0][0]                 \n","__________________________________________________________________________________________________\n","conv_dw_13_relu (ReLU)          (None, 7, 7, 1024)   0           conv_dw_13_bn[0][0]              \n","__________________________________________________________________________________________________\n","conv_pw_13 (Conv2D)             (None, 7, 7, 1024)   1048576     conv_dw_13_relu[0][0]            \n","__________________________________________________________________________________________________\n","conv_pw_13_bn (BatchNormalizati (None, 7, 7, 1024)   4096        conv_pw_13[0][0]                 \n","__________________________________________________________________________________________________\n","conv_pw_13_relu (ReLU)          (None, 7, 7, 1024)   0           conv_pw_13_bn[0][0]              \n","__________________________________________________________________________________________________\n","up_sampling2d_20 (UpSampling2D) (None, 14, 14, 1024) 0           conv_pw_13_relu[0][0]            \n","__________________________________________________________________________________________________\n","concatenate_20 (Concatenate)    (None, 14, 14, 1536) 0           up_sampling2d_20[0][0]           \n","                                                                 conv_pw_11_relu[0][0]            \n","__________________________________________________________________________________________________\n","up_sampling2d_21 (UpSampling2D) (None, 28, 28, 1536) 0           concatenate_20[0][0]             \n","__________________________________________________________________________________________________\n","concatenate_21 (Concatenate)    (None, 28, 28, 1792) 0           up_sampling2d_21[0][0]           \n","                                                                 conv_pw_5_relu[0][0]             \n","__________________________________________________________________________________________________\n","up_sampling2d_22 (UpSampling2D) (None, 56, 56, 1792) 0           concatenate_21[0][0]             \n","__________________________________________________________________________________________________\n","concatenate_22 (Concatenate)    (None, 56, 56, 1920) 0           up_sampling2d_22[0][0]           \n","                                                                 conv_pw_2_relu[0][0]             \n","__________________________________________________________________________________________________\n","up_sampling2d_23 (UpSampling2D) (None, 112, 112, 192 0           concatenate_22[0][0]             \n","__________________________________________________________________________________________________\n","concatenate_23 (Concatenate)    (None, 112, 112, 198 0           up_sampling2d_23[0][0]           \n","                                                                 conv_pw_1_relu[0][0]             \n","__________________________________________________________________________________________________\n","up_sampling2d_24 (UpSampling2D) (None, 224, 224, 198 0           concatenate_23[0][0]             \n","__________________________________________________________________________________________________\n","concatenate_24 (Concatenate)    (None, 224, 224, 198 0           up_sampling2d_24[0][0]           \n","                                                                 input_5[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 224, 224, 1)  1988        concatenate_24[0][0]             \n","__________________________________________________________________________________________________\n","reshape_4 (Reshape)             (None, 224, 224)     0           conv2d_4[0][0]                   \n","==================================================================================================\n","Total params: 3,230,852\n","Trainable params: 1,988\n","Non-trainable params: 3,228,864\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2spcE4TvfEZw"},"source":["### Define dice coefficient function (10 marks)\n","- Create a function to calculate dice coefficient\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8H8aViXZuWz1","colab":{}},"source":["%tensorflow_version 2.x\n","import tensorflow\n","\n","def dice_coefficient(y_true, y_pred):\n","    numerator = 2 * tensorflow.reduce_sum(y_true * y_pred)\n","    denominator = tensorflow.reduce_sum(y_true + y_pred)\n","\n","    return numerator / (denominator + tensorflow.keras.backend.epsilon())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Nkp5SDM1fIu2"},"source":["### Define loss"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FEOVfs19KVLv","colab":{}},"source":["from tensorflow.keras.losses import binary_crossentropy\n","from tensorflow.keras.backend import log, epsilon\n","\n","def loss(y_true, y_pred):\n","    return binary_crossentropy(y_true, y_pred) - log(dice_coefficient(y_true, y_pred) + epsilon())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Thltv_akfOMS"},"source":["### Compile the model (3 marks)\n","- Complie the model using below parameters\n","  - loss: use the loss function defined above\n","  - optimizers: use Adam optimizer\n","  - metrics: use dice_coefficient function defined above"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"atPb8xm2qkK5","colab":{}},"source":["from tensorflow.keras.optimizers import Adam\n","\n","optimizer = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n","model.compile(loss=loss, optimizer=optimizer, metrics=[dice_coefficient])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VTumZyg0fuVy"},"source":["### Define checkpoint and earlystopping"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QNlQHt8DMy7h","colab":{}},"source":["from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","\n","checkpoint = ModelCheckpoint(\"model-{loss:.2f}.h5\", monitor=\"loss\", verbose=1, save_best_only=True,\n","                             save_weights_only=True, mode=\"min\", save_freq=1)\n","stop = EarlyStopping(monitor=\"loss\", patience=5, mode=\"min\")\n","reduce_lr = ReduceLROnPlateau(monitor=\"loss\", factor=0.2, patience=5, min_lr=1e-6, verbose=1, mode=\"min\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LxxbwvXEf07e"},"source":["### Fit the model (3 marks)\n","- Fit the model using below parameters\n","  - epochs: you can decide\n","  - batch_size: 1\n","  - callbacks: checkpoint, reduce_lr, stop"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"guFfKsEmq58j","outputId":"112d9d6a-6fbc-48b5-89c5-5c5b9d4b2f0e","executionInfo":{"status":"ok","timestamp":1589950385437,"user_tz":-330,"elapsed":191237,"user":{"displayName":"Atul Kumar","photoUrl":"","userId":"02267315536838712153"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model.fit(X_train, masks, epochs=5, batch_size=1, callbacks=[checkpoint, stop, reduce_lr])"],"execution_count":56,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","(1, 224, 224)\n","(1, 224, 224)\n","(1, 224, 224)\n","(1, 224, 224)\n","(1, 224, 224)\n","(1, 224, 224)\n","(1, 224, 224)\n","(1, 224, 224)\n","\n","Epoch 00001: loss improved from inf to 4.13784, saving model to model-4.14.h5\n","  1/409 [..............................] - ETA: 0s - loss: 4.1378 - dice_coefficient: 0.0463\n","Epoch 00001: loss improved from 4.13784 to 3.11705, saving model to model-3.12.h5\n","  2/409 [..............................] - ETA: 45s - loss: 3.1171 - dice_coefficient: 0.1283WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.203245). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 3.11705\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.199946). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 3.11705\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.100040). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 3.11705\n","  5/409 [..............................] - ETA: 23s - loss: 3.2008 - dice_coefficient: 0.1054\n","Epoch 00001: loss improved from 3.11705 to 2.90016, saving model to model-2.90.h5\n","  6/409 [..............................] - ETA: 36s - loss: 2.9002 - dice_coefficient: 0.1553WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.100280). Check your callbacks.\n","\n","Epoch 00001: loss improved from 2.90016 to 2.67823, saving model to model-2.68.h5\n","  7/409 [..............................] - ETA: 1:25 - loss: 2.6782 - dice_coefficient: 0.1941WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.199946). Check your callbacks.\n","\n","Epoch 00001: loss improved from 2.67823 to 2.49847, saving model to model-2.50.h5\n","  8/409 [..............................] - ETA: 2:03 - loss: 2.4985 - dice_coefficient: 0.2305WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.203245). Check your callbacks.\n","\n","Epoch 00001: loss improved from 2.49847 to 2.37453, saving model to model-2.37.h5\n","  9/409 [..............................] - ETA: 2:42 - loss: 2.3745 - dice_coefficient: 0.2465WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.206544). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 2.37453\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.203245). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 2.37453\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.100280). Check your callbacks.\n","\n","Epoch 00001: loss improved from 2.37453 to 2.33610, saving model to model-2.34.h5\n"," 12/409 [..............................] - ETA: 2:38 - loss: 2.3361 - dice_coefficient: 0.2441WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.116936). Check your callbacks.\n","\n","Epoch 00001: loss improved from 2.33610 to 2.27360, saving model to model-2.27.h5\n"," 13/409 [..............................] - ETA: 2:39 - loss: 2.2736 - dice_coefficient: 0.2590WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.323218). Check your callbacks.\n","\n","Epoch 00001: loss improved from 2.27360 to 2.22210, saving model to model-2.22.h5\n"," 14/409 [>.............................] - ETA: 2:48 - loss: 2.2221 - dice_coefficient: 0.2686WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.572023). Check your callbacks.\n","\n","Epoch 00001: loss improved from 2.22210 to 2.18886, saving model to model-2.19.h5\n"," 15/409 [>.............................] - ETA: 2:49 - loss: 2.1889 - dice_coefficient: 0.2756WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.592758). Check your callbacks.\n","\n","Epoch 00001: loss improved from 2.18886 to 2.18065, saving model to model-2.18.h5\n"," 16/409 [>.............................] - ETA: 2:49 - loss: 2.1806 - dice_coefficient: 0.2700WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.592758). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 2.18065\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.438015). Check your callbacks.\n","\n","Epoch 00001: loss improved from 2.18065 to 2.15103, saving model to model-2.15.h5\n"," 18/409 [>.............................] - ETA: 2:40 - loss: 2.1510 - dice_coefficient: 0.2678WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.436453). Check your callbacks.\n","\n","Epoch 00001: loss improved from 2.15103 to 2.13962, saving model to model-2.14.h5\n"," 19/409 [>.............................] - ETA: 2:42 - loss: 2.1396 - dice_coefficient: 0.2645WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.436453). Check your callbacks.\n","\n","Epoch 00001: loss improved from 2.13962 to 2.13491, saving model to model-2.13.h5\n"," 20/409 [>.............................] - ETA: 2:42 - loss: 2.1349 - dice_coefficient: 0.2606WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.444105). Check your callbacks.\n","\n","Epoch 00001: loss improved from 2.13491 to 2.11015, saving model to model-2.11.h5\n"," 21/409 [>.............................] - ETA: 2:45 - loss: 2.1101 - dice_coefficient: 0.2599WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.453087). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 2.11015\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.444105). Check your callbacks.\n","\n","Epoch 00001: loss improved from 2.11015 to 2.10804, saving model to model-2.11.h5\n"," 23/409 [>.............................] - ETA: 2:39 - loss: 2.1080 - dice_coefficient: 0.2584WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.453087). Check your callbacks.\n","\n","Epoch 00001: loss improved from 2.10804 to 2.07445, saving model to model-2.07.h5\n"," 24/409 [>.............................] - ETA: 2:41 - loss: 2.0745 - dice_coefficient: 0.2613WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.453087). Check your callbacks.\n","\n","Epoch 00001: loss improved from 2.07445 to 2.06425, saving model to model-2.06.h5\n"," 25/409 [>.............................] - ETA: 2:45 - loss: 2.0643 - dice_coefficient: 0.2604WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.467483). Check your callbacks.\n","\n","Epoch 00001: loss improved from 2.06425 to 2.04913, saving model to model-2.05.h5\n"," 26/409 [>.............................] - ETA: 2:45 - loss: 2.0491 - dice_coefficient: 0.2607WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.469275). Check your callbacks.\n","\n","Epoch 00001: loss improved from 2.04913 to 2.00338, saving model to model-2.00.h5\n"," 27/409 [>.............................] - ETA: 2:46 - loss: 2.0034 - dice_coefficient: 0.2707WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.481529). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 2.00338\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.481529). Check your callbacks.\n","\n","Epoch 00001: loss improved from 2.00338 to 1.97425, saving model to model-1.97.h5\n"," 29/409 [=>............................] - ETA: 2:40 - loss: 1.9743 - dice_coefficient: 0.2717WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.478089). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.97425 to 1.95304, saving model to model-1.95.h5\n"," 30/409 [=>............................] - ETA: 2:41 - loss: 1.9530 - dice_coefficient: 0.2752WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.487788). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.95304\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.478089). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.95304 to 1.93309, saving model to model-1.93.h5\n"," 32/409 [=>............................] - ETA: 2:38 - loss: 1.9331 - dice_coefficient: 0.2766WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.487788). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.93309 to 1.89293, saving model to model-1.89.h5\n"," 33/409 [=>............................] - ETA: 2:39 - loss: 1.8929 - dice_coefficient: 0.2861WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.505721). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.89293 to 1.87127, saving model to model-1.87.h5\n"," 34/409 [=>............................] - ETA: 2:40 - loss: 1.8713 - dice_coefficient: 0.2897WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.498600). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.87127 to 1.85124, saving model to model-1.85.h5\n"," 35/409 [=>............................] - ETA: 2:41 - loss: 1.8512 - dice_coefficient: 0.2951WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.498600). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.85124 to 1.82603, saving model to model-1.83.h5\n"," 36/409 [=>............................] - ETA: 2:42 - loss: 1.8260 - dice_coefficient: 0.3048WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.521286). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.82603 to 1.79691, saving model to model-1.80.h5\n"," 37/409 [=>............................] - ETA: 2:43 - loss: 1.7969 - dice_coefficient: 0.3142WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.528407). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.79691 to 1.78417, saving model to model-1.78.h5\n"," 38/409 [=>............................] - ETA: 2:44 - loss: 1.7842 - dice_coefficient: 0.3146WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.531842). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.78417\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.531842). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.78417 to 1.78259, saving model to model-1.78.h5\n"," 40/409 [=>............................] - ETA: 2:40 - loss: 1.7826 - dice_coefficient: 0.3135WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.528423). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.78259 to 1.78223, saving model to model-1.78.h5\n"," 41/409 [==>...........................] - ETA: 2:43 - loss: 1.7822 - dice_coefficient: 0.3136WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.538603). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.78223 to 1.76510, saving model to model-1.77.h5\n"," 42/409 [==>...........................] - ETA: 2:47 - loss: 1.7651 - dice_coefficient: 0.3159WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.538603). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.76510 to 1.75152, saving model to model-1.75.h5\n"," 43/409 [==>...........................] - ETA: 2:45 - loss: 1.7515 - dice_coefficient: 0.3193WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.535168). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.75152 to 1.74412, saving model to model-1.74.h5\n"," 44/409 [==>...........................] - ETA: 2:49 - loss: 1.7441 - dice_coefficient: 0.3188WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.547235). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.74412 to 1.73082, saving model to model-1.73.h5\n"," 45/409 [==>...........................] - ETA: 2:46 - loss: 1.7308 - dice_coefficient: 0.3213WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.535168). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.73082 to 1.72042, saving model to model-1.72.h5\n"," 46/409 [==>...........................] - ETA: 2:50 - loss: 1.7204 - dice_coefficient: 0.3221WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.548637). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.72042\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.534702). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.72042 to 1.71642, saving model to model-1.72.h5\n"," 48/409 [==>...........................] - ETA: 2:46 - loss: 1.7164 - dice_coefficient: 0.3198WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.500870). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.71642 to 1.70127, saving model to model-1.70.h5\n"," 49/409 [==>...........................] - ETA: 2:51 - loss: 1.7013 - dice_coefficient: 0.3214WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.675727). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.70127 to 1.70042, saving model to model-1.70.h5\n"," 50/409 [==>...........................] - ETA: 2:51 - loss: 1.7004 - dice_coefficient: 0.3206WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.666397). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.70042 to 1.68781, saving model to model-1.69.h5\n"," 51/409 [==>...........................] - ETA: 2:54 - loss: 1.6878 - dice_coefficient: 0.3222WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.720611). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.68781\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.491539). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.68781 to 1.68615, saving model to model-1.69.h5\n"," 53/409 [==>...........................] - ETA: 2:51 - loss: 1.6862 - dice_coefficient: 0.3204WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.527777). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.68615 to 1.67371, saving model to model-1.67.h5\n"," 54/409 [==>...........................] - ETA: 2:55 - loss: 1.6737 - dice_coefficient: 0.3239WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.527777). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.67371 to 1.65927, saving model to model-1.66.h5\n"," 55/409 [===>..........................] - ETA: 3:00 - loss: 1.6593 - dice_coefficient: 0.3281WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.742991). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.65927 to 1.65165, saving model to model-1.65.h5\n"," 56/409 [===>..........................] - ETA: 3:04 - loss: 1.6517 - dice_coefficient: 0.3293WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.742991). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.65165 to 1.64636, saving model to model-1.65.h5\n"," 57/409 [===>..........................] - ETA: 3:05 - loss: 1.6464 - dice_coefficient: 0.3296WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.805673). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.64636 to 1.64265, saving model to model-1.64.h5\n"," 58/409 [===>..........................] - ETA: 3:07 - loss: 1.6427 - dice_coefficient: 0.3303WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.963294). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.64265 to 1.63335, saving model to model-1.63.h5\n"," 59/409 [===>..........................] - ETA: 3:08 - loss: 1.6333 - dice_coefficient: 0.3335WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.876529). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.63335\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.876529). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.63335\n"," 61/409 [===>..........................] - ETA: 3:01 - loss: 1.6520 - dice_coefficient: 0.3338WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.746377). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.63335\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.746377). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.63335\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.746377). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.63335 to 1.61684, saving model to model-1.62.h5\n"," 64/409 [===>..........................] - ETA: 2:58 - loss: 1.6168 - dice_coefficient: 0.3421WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.746377). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.61684 to 1.60821, saving model to model-1.61.h5\n"," 65/409 [===>..........................] - ETA: 3:02 - loss: 1.6082 - dice_coefficient: 0.3433WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.746377). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.60821\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.338098). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.60821\n","\n","Epoch 00001: loss did not improve from 1.60821\n"," 68/409 [===>..........................] - ETA: 2:53 - loss: 1.6119 - dice_coefficient: 0.3395\n","Epoch 00001: loss did not improve from 1.60821\n","\n","Epoch 00001: loss improved from 1.60821 to 1.60532, saving model to model-1.61.h5\n"," 70/409 [====>.........................] - ETA: 2:50 - loss: 1.6053 - dice_coefficient: 0.3392\n","Epoch 00001: loss improved from 1.60532 to 1.60024, saving model to model-1.60.h5\n"," 71/409 [====>.........................] - ETA: 2:51 - loss: 1.6002 - dice_coefficient: 0.3410\n","Epoch 00001: loss did not improve from 1.60024\n","\n","Epoch 00001: loss did not improve from 1.60024\n","\n","Epoch 00001: loss did not improve from 1.60024\n"," 74/409 [====>.........................] - ETA: 2:43 - loss: 1.6073 - dice_coefficient: 0.3387\n","Epoch 00001: loss improved from 1.60024 to 1.59386, saving model to model-1.59.h5\n"," 75/409 [====>.........................] - ETA: 2:46 - loss: 1.5939 - dice_coefficient: 0.3434\n","Epoch 00001: loss improved from 1.59386 to 1.58142, saving model to model-1.58.h5\n"," 76/409 [====>.........................] - ETA: 2:50 - loss: 1.5814 - dice_coefficient: 0.3460\n","Epoch 00001: loss improved from 1.58142 to 1.57192, saving model to model-1.57.h5\n"," 77/409 [====>.........................] - ETA: 2:52 - loss: 1.5719 - dice_coefficient: 0.3487WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.251475). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.57192\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.251475). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.57192\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.251475). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.57192 to 1.57135, saving model to model-1.57.h5\n"," 80/409 [====>.........................] - ETA: 2:46 - loss: 1.5714 - dice_coefficient: 0.3483WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.237242). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.57135 to 1.56601, saving model to model-1.57.h5\n"," 81/409 [====>.........................] - ETA: 2:46 - loss: 1.5660 - dice_coefficient: 0.3497WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.237242). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.56601 to 1.56169, saving model to model-1.56.h5\n"," 82/409 [=====>........................] - ETA: 2:47 - loss: 1.5617 - dice_coefficient: 0.3499WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.521763). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.56169 to 1.55360, saving model to model-1.55.h5\n"," 83/409 [=====>........................] - ETA: 2:50 - loss: 1.5536 - dice_coefficient: 0.3519WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.779149). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.55360\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.779149). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.55360 to 1.54796, saving model to model-1.55.h5\n"," 85/409 [=====>........................] - ETA: 2:47 - loss: 1.5480 - dice_coefficient: 0.3530WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.534127). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.54796 to 1.53888, saving model to model-1.54.h5\n"," 86/409 [=====>........................] - ETA: 2:50 - loss: 1.5389 - dice_coefficient: 0.3562WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.534127). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.53888 to 1.52922, saving model to model-1.53.h5\n"," 87/409 [=====>........................] - ETA: 2:51 - loss: 1.5292 - dice_coefficient: 0.3594WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.534127). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.52922 to 1.52071, saving model to model-1.52.h5\n"," 88/409 [=====>........................] - ETA: 2:53 - loss: 1.5207 - dice_coefficient: 0.3632WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.758573). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.52071 to 1.51442, saving model to model-1.51.h5\n"," 89/409 [=====>........................] - ETA: 2:54 - loss: 1.5144 - dice_coefficient: 0.3642WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.902049). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.51442 to 1.50359, saving model to model-1.50.h5\n"," 90/409 [=====>........................] - ETA: 3:09 - loss: 1.5036 - dice_coefficient: 0.3676WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.968014). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.50359 to 1.49697, saving model to model-1.50.h5\n"," 91/409 [=====>........................] - ETA: 3:09 - loss: 1.4970 - dice_coefficient: 0.3694WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.968014). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.49697 to 1.48869, saving model to model-1.49.h5\n"," 92/409 [=====>........................] - ETA: 3:11 - loss: 1.4887 - dice_coefficient: 0.3724WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.096830). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.48869 to 1.48045, saving model to model-1.48.h5\n"," 93/409 [=====>........................] - ETA: 3:12 - loss: 1.4804 - dice_coefficient: 0.3754WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.953420). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.48045 to 1.47125, saving model to model-1.47.h5\n"," 94/409 [=====>........................] - ETA: 3:13 - loss: 1.4713 - dice_coefficient: 0.3785WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.102812). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.47125 to 1.46964, saving model to model-1.47.h5\n"," 95/409 [=====>........................] - ETA: 3:12 - loss: 1.4696 - dice_coefficient: 0.3775WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.102812). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.46964\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.953420). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.46964 to 1.46672, saving model to model-1.47.h5\n"," 97/409 [======>.......................] - ETA: 3:09 - loss: 1.4667 - dice_coefficient: 0.3783WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.908031). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.46672 to 1.46230, saving model to model-1.46.h5\n"," 98/409 [======>.......................] - ETA: 3:09 - loss: 1.4623 - dice_coefficient: 0.3791WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.896317). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.46230\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.730278). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.46230 to 1.46148, saving model to model-1.46.h5\n","100/409 [======>.......................] - ETA: 3:06 - loss: 1.4615 - dice_coefficient: 0.3793WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.513663). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.46148 to 1.45995, saving model to model-1.46.h5\n","101/409 [======>.......................] - ETA: 3:05 - loss: 1.4599 - dice_coefficient: 0.3812WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.516828). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.45995 to 1.45464, saving model to model-1.45.h5\n","102/409 [======>.......................] - ETA: 3:06 - loss: 1.4546 - dice_coefficient: 0.3826WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.516828). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.45464\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.497155). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.45464 to 1.45342, saving model to model-1.45.h5\n","104/409 [======>.......................] - ETA: 3:02 - loss: 1.4534 - dice_coefficient: 0.3820WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.480196). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.45342 to 1.44690, saving model to model-1.45.h5\n","105/409 [======>.......................] - ETA: 3:01 - loss: 1.4469 - dice_coefficient: 0.3834WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.480196). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.44690\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.480196). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.44690\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.370615). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.44690\n","108/409 [======>.......................] - ETA: 2:54 - loss: 1.4489 - dice_coefficient: 0.3812WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.128455). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.44690 to 1.44234, saving model to model-1.44.h5\n","109/409 [======>.......................] - ETA: 2:55 - loss: 1.4423 - dice_coefficient: 0.3830WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.370615). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.44234\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.128455). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.44234\n","\n","Epoch 00001: loss improved from 1.44234 to 1.44099, saving model to model-1.44.h5\n","112/409 [=======>......................] - ETA: 2:50 - loss: 1.4410 - dice_coefficient: 0.3839\n","Epoch 00001: loss did not improve from 1.44099\n","\n","Epoch 00001: loss did not improve from 1.44099\n","\n","Epoch 00001: loss did not improve from 1.44099\n","115/409 [=======>......................] - ETA: 2:44 - loss: 1.4544 - dice_coefficient: 0.3821\n","Epoch 00001: loss did not improve from 1.44099\n","\n","Epoch 00001: loss did not improve from 1.44099\n","\n","Epoch 00001: loss did not improve from 1.44099\n","118/409 [=======>......................] - ETA: 2:38 - loss: 1.4543 - dice_coefficient: 0.3803\n","Epoch 00001: loss did not improve from 1.44099\n","\n","Epoch 00001: loss did not improve from 1.44099\n","\n","Epoch 00001: loss did not improve from 1.44099\n","121/409 [=======>......................] - ETA: 2:33 - loss: 1.4577 - dice_coefficient: 0.3807\n","Epoch 00001: loss did not improve from 1.44099\n","\n","Epoch 00001: loss did not improve from 1.44099\n","\n","Epoch 00001: loss did not improve from 1.44099\n","124/409 [========>.....................] - ETA: 2:28 - loss: 1.4584 - dice_coefficient: 0.3828\n","Epoch 00001: loss did not improve from 1.44099\n","\n","Epoch 00001: loss did not improve from 1.44099\n","\n","Epoch 00001: loss did not improve from 1.44099\n","127/409 [========>.....................] - ETA: 2:23 - loss: 1.4571 - dice_coefficient: 0.3830\n","Epoch 00001: loss did not improve from 1.44099\n","\n","Epoch 00001: loss did not improve from 1.44099\n","\n","Epoch 00001: loss did not improve from 1.44099\n","130/409 [========>.....................] - ETA: 2:18 - loss: 1.4499 - dice_coefficient: 0.3843\n","Epoch 00001: loss did not improve from 1.44099\n","\n","Epoch 00001: loss did not improve from 1.44099\n","\n","Epoch 00001: loss did not improve from 1.44099\n","133/409 [========>.....................] - ETA: 2:14 - loss: 1.4456 - dice_coefficient: 0.3853\n","Epoch 00001: loss did not improve from 1.44099\n","\n","Epoch 00001: loss did not improve from 1.44099\n","\n","Epoch 00001: loss did not improve from 1.44099\n","136/409 [========>.....................] - ETA: 2:10 - loss: 1.4484 - dice_coefficient: 0.3861\n","Epoch 00001: loss did not improve from 1.44099\n","\n","Epoch 00001: loss did not improve from 1.44099\n","\n","Epoch 00001: loss did not improve from 1.44099\n","139/409 [=========>....................] - ETA: 2:05 - loss: 1.4412 - dice_coefficient: 0.3893\n","Epoch 00001: loss improved from 1.44099 to 1.44087, saving model to model-1.44.h5\n","140/409 [=========>....................] - ETA: 2:04 - loss: 1.4409 - dice_coefficient: 0.3890\n","Epoch 00001: loss did not improve from 1.44087\n","\n","Epoch 00001: loss improved from 1.44087 to 1.43735, saving model to model-1.44.h5\n","142/409 [=========>....................] - ETA: 2:03 - loss: 1.4373 - dice_coefficient: 0.3903\n","Epoch 00001: loss improved from 1.43735 to 1.43142, saving model to model-1.43.h5\n","143/409 [=========>....................] - ETA: 2:04 - loss: 1.4314 - dice_coefficient: 0.3925\n","Epoch 00001: loss improved from 1.43142 to 1.42880, saving model to model-1.43.h5\n","144/409 [=========>....................] - ETA: 2:03 - loss: 1.4288 - dice_coefficient: 0.3926\n","Epoch 00001: loss improved from 1.42880 to 1.42371, saving model to model-1.42.h5\n","145/409 [=========>....................] - ETA: 2:04 - loss: 1.4237 - dice_coefficient: 0.3937\n","Epoch 00001: loss improved from 1.42371 to 1.41772, saving model to model-1.42.h5\n","146/409 [=========>....................] - ETA: 2:03 - loss: 1.4177 - dice_coefficient: 0.3962WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.338462). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.41772\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.338462). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.41772 to 1.41595, saving model to model-1.42.h5\n","148/409 [=========>....................] - ETA: 2:02 - loss: 1.4160 - dice_coefficient: 0.3963WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.492061). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.41595 to 1.41305, saving model to model-1.41.h5\n","149/409 [=========>....................] - ETA: 2:02 - loss: 1.4131 - dice_coefficient: 0.3975WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.503429). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.41305 to 1.41185, saving model to model-1.41.h5\n","150/409 [==========>...................] - ETA: 2:01 - loss: 1.4118 - dice_coefficient: 0.3972WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.503429). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.41185 to 1.40837, saving model to model-1.41.h5\n","151/409 [==========>...................] - ETA: 2:00 - loss: 1.4084 - dice_coefficient: 0.3981WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.503429). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.40837 to 1.40281, saving model to model-1.40.h5\n","152/409 [==========>...................] - ETA: 2:01 - loss: 1.4028 - dice_coefficient: 0.3999WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.510985). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.40281\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.496464). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.40281\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.344432). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.40281 to 1.39917, saving model to model-1.40.h5\n","155/409 [==========>...................] - ETA: 1:58 - loss: 1.3992 - dice_coefficient: 0.4010WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.332438). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.39917 to 1.39751, saving model to model-1.40.h5\n","156/409 [==========>...................] - ETA: 1:58 - loss: 1.3975 - dice_coefficient: 0.4008WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.332438). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.39751 to 1.39210, saving model to model-1.39.h5\n","157/409 [==========>...................] - ETA: 1:58 - loss: 1.3921 - dice_coefficient: 0.4027WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.473102). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.39210 to 1.38628, saving model to model-1.39.h5\n","158/409 [==========>...................] - ETA: 1:58 - loss: 1.3863 - dice_coefficient: 0.4043WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.479480). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.38628 to 1.38370, saving model to model-1.38.h5\n","159/409 [==========>...................] - ETA: 1:58 - loss: 1.3837 - dice_coefficient: 0.4051WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.479480). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.38370 to 1.38129, saving model to model-1.38.h5\n","160/409 [==========>...................] - ETA: 1:58 - loss: 1.3813 - dice_coefficient: 0.4050WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.502552). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.38129 to 1.37863, saving model to model-1.38.h5\n","161/409 [==========>...................] - ETA: 1:58 - loss: 1.3786 - dice_coefficient: 0.4058WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.508136). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.37863 to 1.37370, saving model to model-1.37.h5\n","162/409 [==========>...................] - ETA: 1:58 - loss: 1.3737 - dice_coefficient: 0.4069WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.508136). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.37370 to 1.37060, saving model to model-1.37.h5\n","163/409 [==========>...................] - ETA: 1:58 - loss: 1.3706 - dice_coefficient: 0.4084WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.508136). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.37060 to 1.36760, saving model to model-1.37.h5\n","164/409 [===========>..................] - ETA: 1:58 - loss: 1.3676 - dice_coefficient: 0.4091WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.522936). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.36760 to 1.36320, saving model to model-1.36.h5\n","165/409 [===========>..................] - ETA: 1:58 - loss: 1.3632 - dice_coefficient: 0.4104WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.541657). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.36320 to 1.36208, saving model to model-1.36.h5\n","166/409 [===========>..................] - ETA: 1:57 - loss: 1.3621 - dice_coefficient: 0.4101WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.541657). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.36208\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.522936). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.36208\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.522936). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.36208\n","169/409 [===========>..................] - ETA: 1:54 - loss: 1.3652 - dice_coefficient: 0.4089WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.506738). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.36208 to 1.36202, saving model to model-1.36.h5\n","170/409 [===========>..................] - ETA: 1:54 - loss: 1.3620 - dice_coefficient: 0.4102WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.500672). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.36202 to 1.35924, saving model to model-1.36.h5\n","171/409 [===========>..................] - ETA: 1:53 - loss: 1.3592 - dice_coefficient: 0.4107WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.500672). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.35924\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.476001). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.35924 to 1.35906, saving model to model-1.36.h5\n","173/409 [===========>..................] - ETA: 1:52 - loss: 1.3591 - dice_coefficient: 0.4105WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.471940). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.35906 to 1.35433, saving model to model-1.35.h5\n","174/409 [===========>..................] - ETA: 1:52 - loss: 1.3543 - dice_coefficient: 0.4118WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.471940). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.35433 to 1.35336, saving model to model-1.35.h5\n","175/409 [===========>..................] - ETA: 1:51 - loss: 1.3534 - dice_coefficient: 0.4119WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.321003). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.35336 to 1.34870, saving model to model-1.35.h5\n","176/409 [===========>..................] - ETA: 1:51 - loss: 1.3487 - dice_coefficient: 0.4135WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.321003). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.34870 to 1.34380, saving model to model-1.34.h5\n","177/409 [===========>..................] - ETA: 1:51 - loss: 1.3438 - dice_coefficient: 0.4150WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.471940). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.34380\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.471940). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.34380 to 1.34208, saving model to model-1.34.h5\n","179/409 [============>.................] - ETA: 1:49 - loss: 1.3421 - dice_coefficient: 0.4150WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.491229). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.34208 to 1.33694, saving model to model-1.34.h5\n","180/409 [============>.................] - ETA: 1:49 - loss: 1.3369 - dice_coefficient: 0.4169WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.497127). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.33694 to 1.33409, saving model to model-1.33.h5\n","181/409 [============>.................] - ETA: 1:49 - loss: 1.3341 - dice_coefficient: 0.4175WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.497127). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.33409\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.497127). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.33409\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.497127). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.33409\n","184/409 [============>.................] - ETA: 1:46 - loss: 1.3355 - dice_coefficient: 0.4190WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.340292). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.33409 to 1.33208, saving model to model-1.33.h5\n","185/409 [============>.................] - ETA: 1:45 - loss: 1.3321 - dice_coefficient: 0.4203WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.470040). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.33208 to 1.32945, saving model to model-1.33.h5\n","186/409 [============>.................] - ETA: 1:45 - loss: 1.3294 - dice_coefficient: 0.4210WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.470040). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.32945 to 1.32883, saving model to model-1.33.h5\n","187/409 [============>.................] - ETA: 1:45 - loss: 1.3288 - dice_coefficient: 0.4213WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.470040). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.32883 to 1.32826, saving model to model-1.33.h5\n","188/409 [============>.................] - ETA: 1:44 - loss: 1.3283 - dice_coefficient: 0.4214WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.497127). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.32826\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.473475). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.32826\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.223510). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.32826 to 1.32662, saving model to model-1.33.h5\n","191/409 [=============>................] - ETA: 1:42 - loss: 1.3266 - dice_coefficient: 0.4229WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.223510). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.32662\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.223510). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.32662 to 1.32293, saving model to model-1.32.h5\n","193/409 [=============>................] - ETA: 1:41 - loss: 1.3229 - dice_coefficient: 0.4237WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.456264). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.32293 to 1.31829, saving model to model-1.32.h5\n","194/409 [=============>................] - ETA: 1:41 - loss: 1.3183 - dice_coefficient: 0.4252WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.485708). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.31829 to 1.31571, saving model to model-1.32.h5\n","195/409 [=============>................] - ETA: 1:40 - loss: 1.3157 - dice_coefficient: 0.4263WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.505517). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.31571 to 1.31233, saving model to model-1.31.h5\n","196/409 [=============>................] - ETA: 1:40 - loss: 1.3123 - dice_coefficient: 0.4270WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.505517). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.31233 to 1.30821, saving model to model-1.31.h5\n","197/409 [=============>................] - ETA: 1:40 - loss: 1.3082 - dice_coefficient: 0.4283WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.515585). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.30821 to 1.30541, saving model to model-1.31.h5\n","198/409 [=============>................] - ETA: 1:39 - loss: 1.3054 - dice_coefficient: 0.4287WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.485950). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.30541 to 1.30225, saving model to model-1.30.h5\n","199/409 [=============>................] - ETA: 1:39 - loss: 1.3022 - dice_coefficient: 0.4294WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.515585). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.30225 to 1.30014, saving model to model-1.30.h5\n","200/409 [=============>................] - ETA: 1:39 - loss: 1.3001 - dice_coefficient: 0.4295WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.521550). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.30014 to 1.29869, saving model to model-1.30.h5\n","201/409 [=============>................] - ETA: 1:38 - loss: 1.2987 - dice_coefficient: 0.4297WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.521550). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.29869 to 1.29516, saving model to model-1.30.h5\n","202/409 [=============>................] - ETA: 1:38 - loss: 1.2952 - dice_coefficient: 0.4309WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.521550). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.29516 to 1.29394, saving model to model-1.29.h5\n","203/409 [=============>................] - ETA: 1:38 - loss: 1.2939 - dice_coefficient: 0.4307WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.521550). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.29394 to 1.29045, saving model to model-1.29.h5\n","204/409 [=============>................] - ETA: 1:38 - loss: 1.2904 - dice_coefficient: 0.4321WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.540494). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.29045\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.521550). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.29045\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.515240). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.29045\n","207/409 [==============>...............] - ETA: 1:35 - loss: 1.2944 - dice_coefficient: 0.4324WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.474746). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.29045\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.474746). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.29045\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.218653). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.29045\n","210/409 [==============>...............] - ETA: 1:32 - loss: 1.2963 - dice_coefficient: 0.4314\n","Epoch 00001: loss did not improve from 1.29045\n","\n","Epoch 00001: loss did not improve from 1.29045\n","\n","Epoch 00001: loss improved from 1.29045 to 1.28994, saving model to model-1.29.h5\n","213/409 [==============>...............] - ETA: 1:30 - loss: 1.2899 - dice_coefficient: 0.4337\n","Epoch 00001: loss improved from 1.28994 to 1.28759, saving model to model-1.29.h5\n","214/409 [==============>...............] - ETA: 1:29 - loss: 1.2876 - dice_coefficient: 0.4351\n","Epoch 00001: loss improved from 1.28759 to 1.28394, saving model to model-1.28.h5\n","215/409 [==============>...............] - ETA: 1:30 - loss: 1.2839 - dice_coefficient: 0.4364\n","Epoch 00001: loss improved from 1.28394 to 1.28042, saving model to model-1.28.h5\n","216/409 [==============>...............] - ETA: 1:29 - loss: 1.2804 - dice_coefficient: 0.4374\n","Epoch 00001: loss improved from 1.28042 to 1.27867, saving model to model-1.28.h5\n","217/409 [==============>...............] - ETA: 1:29 - loss: 1.2787 - dice_coefficient: 0.4379WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.175686). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.27867 to 1.27662, saving model to model-1.28.h5\n","218/409 [==============>...............] - ETA: 1:28 - loss: 1.2766 - dice_coefficient: 0.4382WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.408430). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.27662 to 1.27578, saving model to model-1.28.h5\n","219/409 [===============>..............] - ETA: 1:28 - loss: 1.2758 - dice_coefficient: 0.4384WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.484355). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.27578 to 1.27478, saving model to model-1.27.h5\n","220/409 [===============>..............] - ETA: 1:28 - loss: 1.2748 - dice_coefficient: 0.4383WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.506796). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.27478 to 1.27221, saving model to model-1.27.h5\n","221/409 [===============>..............] - ETA: 1:27 - loss: 1.2722 - dice_coefficient: 0.4395WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.518002). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.27221 to 1.27150, saving model to model-1.27.h5\n","222/409 [===============>..............] - ETA: 1:27 - loss: 1.2715 - dice_coefficient: 0.4395WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.518002). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.27150\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.518002). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.27150\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.513810). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.27150 to 1.27112, saving model to model-1.27.h5\n","225/409 [===============>..............] - ETA: 1:24 - loss: 1.2711 - dice_coefficient: 0.4398WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.484355). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.27112 to 1.27053, saving model to model-1.27.h5\n","226/409 [===============>..............] - ETA: 1:24 - loss: 1.2705 - dice_coefficient: 0.4398WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.483856). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.27053 to 1.26911, saving model to model-1.27.h5\n","227/409 [===============>..............] - ETA: 1:24 - loss: 1.2691 - dice_coefficient: 0.4402WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.502105). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.26911 to 1.26690, saving model to model-1.27.h5\n","228/409 [===============>..............] - ETA: 1:23 - loss: 1.2669 - dice_coefficient: 0.4406WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.512097). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.26690 to 1.26458, saving model to model-1.26.h5\n","229/409 [===============>..............] - ETA: 1:23 - loss: 1.2646 - dice_coefficient: 0.4420WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.512097). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.26458 to 1.26457, saving model to model-1.26.h5\n","230/409 [===============>..............] - ETA: 1:23 - loss: 1.2646 - dice_coefficient: 0.4418WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.507302). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.26457 to 1.26209, saving model to model-1.26.h5\n","231/409 [===============>..............] - ETA: 1:22 - loss: 1.2621 - dice_coefficient: 0.4426WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.507302). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.26209 to 1.25964, saving model to model-1.26.h5\n","232/409 [================>.............] - ETA: 1:22 - loss: 1.2596 - dice_coefficient: 0.4439WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.515400). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.25964 to 1.25769, saving model to model-1.26.h5\n","233/409 [================>.............] - ETA: 1:22 - loss: 1.2577 - dice_coefficient: 0.4446WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.515400). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.25769 to 1.25498, saving model to model-1.25.h5\n","234/409 [================>.............] - ETA: 1:21 - loss: 1.2550 - dice_coefficient: 0.4459WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.520195). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.25498 to 1.25306, saving model to model-1.25.h5\n","235/409 [================>.............] - ETA: 1:21 - loss: 1.2531 - dice_coefficient: 0.4466WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.533656). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.25306\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.533656). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.25306\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.520195). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.25306 to 1.25155, saving model to model-1.25.h5\n","238/409 [================>.............] - ETA: 1:19 - loss: 1.2515 - dice_coefficient: 0.4463WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.515400). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.25155 to 1.25020, saving model to model-1.25.h5\n","239/409 [================>.............] - ETA: 1:19 - loss: 1.2502 - dice_coefficient: 0.4463WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.515400). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.25020 to 1.24992, saving model to model-1.25.h5\n","240/409 [================>.............] - ETA: 1:18 - loss: 1.2499 - dice_coefficient: 0.4466WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.509914). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.24992\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.501918). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.24992 to 1.24797, saving model to model-1.25.h5\n","242/409 [================>.............] - ETA: 1:17 - loss: 1.2480 - dice_coefficient: 0.4480WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.501918). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.24797 to 1.24590, saving model to model-1.25.h5\n","243/409 [================>.............] - ETA: 1:17 - loss: 1.2459 - dice_coefficient: 0.4489WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.503224). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.24590 to 1.24348, saving model to model-1.24.h5\n","244/409 [================>.............] - ETA: 1:17 - loss: 1.2435 - dice_coefficient: 0.4493WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.503224). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.24348\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.488065). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.24348 to 1.24197, saving model to model-1.24.h5\n","246/409 [=================>............] - ETA: 1:15 - loss: 1.2420 - dice_coefficient: 0.4493WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.488065). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.24197 to 1.23983, saving model to model-1.24.h5\n","247/409 [=================>............] - ETA: 1:15 - loss: 1.2398 - dice_coefficient: 0.4502WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.488065). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.23983 to 1.23693, saving model to model-1.24.h5\n","248/409 [=================>............] - ETA: 1:14 - loss: 1.2369 - dice_coefficient: 0.4510WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.503224). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.23693 to 1.23433, saving model to model-1.23.h5\n","249/409 [=================>............] - ETA: 1:14 - loss: 1.2343 - dice_coefficient: 0.4517WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.503224). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.23433 to 1.23183, saving model to model-1.23.h5\n","250/409 [=================>............] - ETA: 1:14 - loss: 1.2318 - dice_coefficient: 0.4525WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.505420). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.23183 to 1.22958, saving model to model-1.23.h5\n","251/409 [=================>............] - ETA: 1:13 - loss: 1.2296 - dice_coefficient: 0.4530WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.506678). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.22958\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.506678). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.22958 to 1.22940, saving model to model-1.23.h5\n","253/409 [=================>............] - ETA: 1:12 - loss: 1.2294 - dice_coefficient: 0.4544WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.506678). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.22940 to 1.22636, saving model to model-1.23.h5\n","254/409 [=================>............] - ETA: 1:12 - loss: 1.2264 - dice_coefficient: 0.4556WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.506678). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.22636\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.506678). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.22636 to 1.22450, saving model to model-1.22.h5\n","256/409 [=================>............] - ETA: 1:11 - loss: 1.2245 - dice_coefficient: 0.4564WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.511238). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.22450 to 1.22357, saving model to model-1.22.h5\n","257/409 [=================>............] - ETA: 1:10 - loss: 1.2236 - dice_coefficient: 0.4567WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.516480). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.22357 to 1.22075, saving model to model-1.22.h5\n","258/409 [=================>............] - ETA: 1:10 - loss: 1.2208 - dice_coefficient: 0.4576WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.516399). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.22075 to 1.21835, saving model to model-1.22.h5\n","259/409 [=================>............] - ETA: 1:09 - loss: 1.2183 - dice_coefficient: 0.4585WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.514673). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.21835 to 1.21495, saving model to model-1.21.h5\n","260/409 [==================>...........] - ETA: 1:09 - loss: 1.2149 - dice_coefficient: 0.4597WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.516399). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.21495 to 1.21298, saving model to model-1.21.h5\n","261/409 [==================>...........] - ETA: 1:09 - loss: 1.2130 - dice_coefficient: 0.4604WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.517339). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.21298 to 1.21062, saving model to model-1.21.h5\n","262/409 [==================>...........] - ETA: 1:08 - loss: 1.2106 - dice_coefficient: 0.4610WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.517339). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.21062 to 1.20825, saving model to model-1.21.h5\n","263/409 [==================>...........] - ETA: 1:08 - loss: 1.2083 - dice_coefficient: 0.4620WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.516399). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.20825 to 1.20603, saving model to model-1.21.h5\n","264/409 [==================>...........] - ETA: 1:07 - loss: 1.2060 - dice_coefficient: 0.4627WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.523950). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.20603 to 1.20380, saving model to model-1.20.h5\n","265/409 [==================>...........] - ETA: 1:07 - loss: 1.2038 - dice_coefficient: 0.4637WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.533944). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.20380\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.523950). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.20380\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.515533). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.20380\n","268/409 [==================>...........] - ETA: 1:05 - loss: 1.2070 - dice_coefficient: 0.4621WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.513162). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.20380\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.508361). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.20380\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.252405). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.20380 to 1.20170, saving model to model-1.20.h5\n","271/409 [==================>...........] - ETA: 1:03 - loss: 1.2017 - dice_coefficient: 0.4635WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.103462). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.20170 to 1.20082, saving model to model-1.20.h5\n","272/409 [==================>...........] - ETA: 1:03 - loss: 1.2008 - dice_coefficient: 0.4638WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.103462). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.20082 to 1.20012, saving model to model-1.20.h5\n","273/409 [===================>..........] - ETA: 1:02 - loss: 1.2001 - dice_coefficient: 0.4640WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.103462). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.20012 to 1.19926, saving model to model-1.20.h5\n","274/409 [===================>..........] - ETA: 1:02 - loss: 1.1993 - dice_coefficient: 0.4646WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.103462). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.19926\n","\n","Epoch 00001: loss did not improve from 1.19926\n","\n","Epoch 00001: loss did not improve from 1.19926\n","277/409 [===================>..........] - ETA: 1:00 - loss: 1.1998 - dice_coefficient: 0.4643\n","Epoch 00001: loss improved from 1.19926 to 1.19912, saving model to model-1.20.h5\n","278/409 [===================>..........] - ETA: 59s - loss: 1.1991 - dice_coefficient: 0.4643 WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.103521). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.19912 to 1.19742, saving model to model-1.20.h5\n","279/409 [===================>..........] - ETA: 59s - loss: 1.1974 - dice_coefficient: 0.4650WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.325021). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.19742 to 1.19699, saving model to model-1.20.h5\n","280/409 [===================>..........] - ETA: 59s - loss: 1.1970 - dice_coefficient: 0.4655WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.457066). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.19699 to 1.19473, saving model to model-1.19.h5\n","281/409 [===================>..........] - ETA: 58s - loss: 1.1947 - dice_coefficient: 0.4665WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.491493). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.19473 to 1.19280, saving model to model-1.19.h5\n","282/409 [===================>..........] - ETA: 58s - loss: 1.1928 - dice_coefficient: 0.4668WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.485127). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.19280 to 1.19077, saving model to model-1.19.h5\n","283/409 [===================>..........] - ETA: 58s - loss: 1.1908 - dice_coefficient: 0.4675WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.485127). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.19077\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.471785). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.19077 to 1.19060, saving model to model-1.19.h5\n","285/409 [===================>..........] - ETA: 56s - loss: 1.1906 - dice_coefficient: 0.4670WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.495773). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.19060 to 1.19050, saving model to model-1.19.h5\n","286/409 [===================>..........] - ETA: 56s - loss: 1.1905 - dice_coefficient: 0.4671WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.509605). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.19050\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.509605). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.19050\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.509605). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.19050 to 1.19026, saving model to model-1.19.h5\n","289/409 [====================>.........] - ETA: 54s - loss: 1.1903 - dice_coefficient: 0.4675WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.495773). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.19026 to 1.18935, saving model to model-1.19.h5\n","290/409 [====================>.........] - ETA: 54s - loss: 1.1894 - dice_coefficient: 0.4679WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.495773). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.18935 to 1.18654, saving model to model-1.19.h5\n","291/409 [====================>.........] - ETA: 53s - loss: 1.1865 - dice_coefficient: 0.4688WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.495773). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.18654 to 1.18513, saving model to model-1.19.h5\n","292/409 [====================>.........] - ETA: 53s - loss: 1.1851 - dice_coefficient: 0.4689WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.501155). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.18513 to 1.18336, saving model to model-1.18.h5\n","293/409 [====================>.........] - ETA: 53s - loss: 1.1834 - dice_coefficient: 0.4695WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.501155). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.18336 to 1.18093, saving model to model-1.18.h5\n","294/409 [====================>.........] - ETA: 52s - loss: 1.1809 - dice_coefficient: 0.4703WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.501155). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.18093\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.482505). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.18093\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.325266). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.18093 to 1.18089, saving model to model-1.18.h5\n","297/409 [====================>.........] - ETA: 51s - loss: 1.1809 - dice_coefficient: 0.4709WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.465877). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.18089\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.465877). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.18089\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.336743). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.18089 to 1.17994, saving model to model-1.18.h5\n","300/409 [=====================>........] - ETA: 49s - loss: 1.1799 - dice_coefficient: 0.4708WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.336743). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.17994 to 1.17915, saving model to model-1.18.h5\n","301/409 [=====================>........] - ETA: 48s - loss: 1.1791 - dice_coefficient: 0.4708WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.336743). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.17915 to 1.17678, saving model to model-1.18.h5\n","302/409 [=====================>........] - ETA: 48s - loss: 1.1768 - dice_coefficient: 0.4719WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.336743). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.17678 to 1.17559, saving model to model-1.18.h5\n","303/409 [=====================>........] - ETA: 48s - loss: 1.1756 - dice_coefficient: 0.4726WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.313675). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.17559 to 1.17391, saving model to model-1.17.h5\n","304/409 [=====================>........] - ETA: 47s - loss: 1.1739 - dice_coefficient: 0.4734WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.454286). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.17391\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.454286). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.17391\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.454286). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.17391\n","307/409 [=====================>........] - ETA: 46s - loss: 1.1740 - dice_coefficient: 0.4735WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.215903). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.17391 to 1.17278, saving model to model-1.17.h5\n","308/409 [=====================>........] - ETA: 45s - loss: 1.1728 - dice_coefficient: 0.4741WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.462716). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.17278\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.462716). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.17278 to 1.17126, saving model to model-1.17.h5\n","310/409 [=====================>........] - ETA: 44s - loss: 1.1713 - dice_coefficient: 0.4747WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.402094). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.17126\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.186779). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.17126 to 1.17026, saving model to model-1.17.h5\n","312/409 [=====================>........] - ETA: 43s - loss: 1.1703 - dice_coefficient: 0.4746WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.186779). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.17026 to 1.16831, saving model to model-1.17.h5\n","313/409 [=====================>........] - ETA: 43s - loss: 1.1683 - dice_coefficient: 0.4753WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.186779). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.16831 to 1.16626, saving model to model-1.17.h5\n","314/409 [======================>.......] - ETA: 42s - loss: 1.1663 - dice_coefficient: 0.4762WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.186779). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.16626 to 1.16618, saving model to model-1.17.h5\n","315/409 [======================>.......] - ETA: 42s - loss: 1.1662 - dice_coefficient: 0.4766WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.415310). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.16618 to 1.16409, saving model to model-1.16.h5\n","316/409 [======================>.......] - ETA: 41s - loss: 1.1641 - dice_coefficient: 0.4772WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.415310). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.16409 to 1.16207, saving model to model-1.16.h5\n","317/409 [======================>.......] - ETA: 41s - loss: 1.1621 - dice_coefficient: 0.4781WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.415310). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.16207\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.279631). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.16207 to 1.16138, saving model to model-1.16.h5\n","319/409 [======================>.......] - ETA: 40s - loss: 1.1614 - dice_coefficient: 0.4785WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.404457). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.16138\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.311119). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.16138 to 1.16119, saving model to model-1.16.h5\n","321/409 [======================>.......] - ETA: 39s - loss: 1.1612 - dice_coefficient: 0.4787WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.446798). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.16119 to 1.15947, saving model to model-1.16.h5\n","322/409 [======================>.......] - ETA: 38s - loss: 1.1595 - dice_coefficient: 0.4793WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.446798). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.15947 to 1.15939, saving model to model-1.16.h5\n","323/409 [======================>.......] - ETA: 38s - loss: 1.1594 - dice_coefficient: 0.4795WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.323371). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.15939 to 1.15697, saving model to model-1.16.h5\n","324/409 [======================>.......] - ETA: 37s - loss: 1.1570 - dice_coefficient: 0.4805WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.323371). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.15697 to 1.15480, saving model to model-1.15.h5\n","325/409 [======================>.......] - ETA: 37s - loss: 1.1548 - dice_coefficient: 0.4813WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.198545). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.15480 to 1.15290, saving model to model-1.15.h5\n","326/409 [======================>.......] - ETA: 36s - loss: 1.1529 - dice_coefficient: 0.4819WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.202026). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.15290\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.202026). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.15290\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.202026). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.15290\n","329/409 [=======================>......] - ETA: 35s - loss: 1.1962 - dice_coefficient: 0.4812WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.182799). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.15290\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.182799). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","332/409 [=======================>......] - ETA: 33s - loss: 1.1944 - dice_coefficient: 0.4811\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","335/409 [=======================>......] - ETA: 32s - loss: 1.1897 - dice_coefficient: 0.4830\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","338/409 [=======================>......] - ETA: 30s - loss: 1.1868 - dice_coefficient: 0.4837\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","341/409 [========================>.....] - ETA: 28s - loss: 1.1823 - dice_coefficient: 0.4853\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","344/409 [========================>.....] - ETA: 27s - loss: 1.1772 - dice_coefficient: 0.4872\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","347/409 [========================>.....] - ETA: 25s - loss: 1.1720 - dice_coefficient: 0.4889\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","350/409 [========================>.....] - ETA: 24s - loss: 1.1666 - dice_coefficient: 0.4909\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","353/409 [========================>.....] - ETA: 23s - loss: 1.1649 - dice_coefficient: 0.4908\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","356/409 [=========================>....] - ETA: 21s - loss: 1.1604 - dice_coefficient: 0.4920\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","359/409 [=========================>....] - ETA: 20s - loss: 1.1592 - dice_coefficient: 0.4917\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss did not improve from 1.15290\n","362/409 [=========================>....] - ETA: 18s - loss: 1.1542 - dice_coefficient: 0.4930\n","Epoch 00001: loss did not improve from 1.15290\n","\n","Epoch 00001: loss improved from 1.15290 to 1.15207, saving model to model-1.15.h5\n","364/409 [=========================>....] - ETA: 18s - loss: 1.1521 - dice_coefficient: 0.4938\n","Epoch 00001: loss improved from 1.15207 to 1.15067, saving model to model-1.15.h5\n","365/409 [=========================>....] - ETA: 17s - loss: 1.1507 - dice_coefficient: 0.4941\n","Epoch 00001: loss improved from 1.15067 to 1.14936, saving model to model-1.15.h5\n","366/409 [=========================>....] - ETA: 17s - loss: 1.1494 - dice_coefficient: 0.4943\n","Epoch 00001: loss improved from 1.14936 to 1.14827, saving model to model-1.15.h5\n","367/409 [=========================>....] - ETA: 16s - loss: 1.1483 - dice_coefficient: 0.4944\n","Epoch 00001: loss improved from 1.14827 to 1.14768, saving model to model-1.15.h5\n","368/409 [=========================>....] - ETA: 16s - loss: 1.1477 - dice_coefficient: 0.4942\n","Epoch 00001: loss did not improve from 1.14768\n","\n","Epoch 00001: loss did not improve from 1.14768\n","\n","Epoch 00001: loss did not improve from 1.14768\n","371/409 [==========================>...] - ETA: 15s - loss: 1.1504 - dice_coefficient: 0.4930\n","Epoch 00001: loss did not improve from 1.14768\n","\n","Epoch 00001: loss did not improve from 1.14768\n","\n","Epoch 00001: loss improved from 1.14768 to 1.14612, saving model to model-1.15.h5\n","374/409 [==========================>...] - ETA: 13s - loss: 1.1461 - dice_coefficient: 0.4945WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.201179). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.14612 to 1.14456, saving model to model-1.14.h5\n","375/409 [==========================>...] - ETA: 13s - loss: 1.1446 - dice_coefficient: 0.4950\n","Epoch 00001: loss improved from 1.14456 to 1.14325, saving model to model-1.14.h5\n","376/409 [==========================>...] - ETA: 13s - loss: 1.1433 - dice_coefficient: 0.4953\n","Epoch 00001: loss improved from 1.14325 to 1.14159, saving model to model-1.14.h5\n","377/409 [==========================>...] - ETA: 12s - loss: 1.1416 - dice_coefficient: 0.4958\n","Epoch 00001: loss improved from 1.14159 to 1.14035, saving model to model-1.14.h5\n","378/409 [==========================>...] - ETA: 12s - loss: 1.1404 - dice_coefficient: 0.4960\n","Epoch 00001: loss improved from 1.14035 to 1.13911, saving model to model-1.14.h5\n","379/409 [==========================>...] - ETA: 11s - loss: 1.1391 - dice_coefficient: 0.4964WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.181572). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.13911 to 1.13758, saving model to model-1.14.h5\n","380/409 [==========================>...] - ETA: 11s - loss: 1.1376 - dice_coefficient: 0.4968WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.188907). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.13758\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.188907). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.13758\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.188907). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.13758 to 1.13719, saving model to model-1.14.h5\n","383/409 [===========================>..] - ETA: 10s - loss: 1.1372 - dice_coefficient: 0.4972WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.195646). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.13719 to 1.13617, saving model to model-1.14.h5\n","384/409 [===========================>..] - ETA: 9s - loss: 1.1362 - dice_coefficient: 0.4974 WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.188907). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.13617\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.188907). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.13617\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.187480). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.13617 to 1.13603, saving model to model-1.14.h5\n","387/409 [===========================>..] - ETA: 8s - loss: 1.1360 - dice_coefficient: 0.4975WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.187480). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.13603 to 1.13439, saving model to model-1.13.h5\n","388/409 [===========================>..] - ETA: 8s - loss: 1.1344 - dice_coefficient: 0.4982WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.184602). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.13439 to 1.13284, saving model to model-1.13.h5\n","389/409 [===========================>..] - ETA: 7s - loss: 1.1328 - dice_coefficient: 0.4986WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.180431). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.13284 to 1.13169, saving model to model-1.13.h5\n","390/409 [===========================>..] - ETA: 7s - loss: 1.1317 - dice_coefficient: 0.4989WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.180431). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.13169 to 1.13043, saving model to model-1.13.h5\n","391/409 [===========================>..] - ETA: 7s - loss: 1.1304 - dice_coefficient: 0.4994WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.184602). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.13043 to 1.12922, saving model to model-1.13.h5\n","392/409 [===========================>..] - ETA: 6s - loss: 1.1292 - dice_coefficient: 0.4997WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.194213). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.12922 to 1.12883, saving model to model-1.13.h5\n","393/409 [===========================>..] - ETA: 6s - loss: 1.1288 - dice_coefficient: 0.4996WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.314155). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.12883\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.312710). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.12883 to 1.12844, saving model to model-1.13.h5\n","395/409 [===========================>..] - ETA: 5s - loss: 1.1284 - dice_coefficient: 0.4994WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.316881). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.12844\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.316881). Check your callbacks.\n","\n","Epoch 00001: loss improved from 1.12844 to 1.12736, saving model to model-1.13.h5\n","397/409 [============================>.] - ETA: 4s - loss: 1.1274 - dice_coefficient: 0.4999WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.194229). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.12736\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.194229). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.12736\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.194229). Check your callbacks.\n","\n","Epoch 00001: loss did not improve from 1.12736\n","400/409 [============================>.] - ETA: 3s - loss: 1.1296 - dice_coefficient: 0.4990\n","Epoch 00001: loss did not improve from 1.12736\n","\n","Epoch 00001: loss did not improve from 1.12736\n","\n","Epoch 00001: loss improved from 1.12736 to 1.12650, saving model to model-1.13.h5\n","403/409 [============================>.] - ETA: 2s - loss: 1.1265 - dice_coefficient: 0.4994\n","Epoch 00001: loss improved from 1.12650 to 1.12524, saving model to model-1.13.h5\n","404/409 [============================>.] - ETA: 1s - loss: 1.1252 - dice_coefficient: 0.4997\n","Epoch 00001: loss improved from 1.12524 to 1.12357, saving model to model-1.12.h5\n","405/409 [============================>.] - ETA: 1s - loss: 1.1236 - dice_coefficient: 0.5001\n","Epoch 00001: loss improved from 1.12357 to 1.12264, saving model to model-1.12.h5\n","406/409 [============================>.] - ETA: 1s - loss: 1.1226 - dice_coefficient: 0.5003\n","Epoch 00001: loss did not improve from 1.12264\n","\n","Epoch 00001: loss did not improve from 1.12264\n","\n","Epoch 00001: loss improved from 1.12264 to 1.12181, saving model to model-1.12.h5\n","409/409 [==============================] - 157s 383ms/step - loss: 1.1218 - dice_coefficient: 0.5008 - lr: 0.0010\n","Epoch 2/5\n","\n","Epoch 00002: loss improved from 1.12181 to 0.57331, saving model to model-0.57.h5\n","  1/409 [..............................] - ETA: 0s - loss: 0.5733 - dice_coefficient: 0.7643WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.184023). Check your callbacks.\n","\n","Epoch 00002: loss improved from 0.57331 to 0.51936, saving model to model-0.52.h5\n","  2/409 [..............................] - ETA: 44s - loss: 0.5194 - dice_coefficient: 0.7318WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.190381). Check your callbacks.\n","\n","Epoch 00002: loss did not improve from 0.51936\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.184023). Check your callbacks.\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","  5/409 [..............................] - ETA: 22s - loss: 0.8525 - dice_coefficient: 0.5984\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","  8/409 [..............................] - ETA: 17s - loss: 0.8539 - dice_coefficient: 0.5874\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 11/409 [..............................] - ETA: 14s - loss: 0.9099 - dice_coefficient: 0.5659\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 14/409 [>.............................] - ETA: 12s - loss: 0.8695 - dice_coefficient: 0.5733\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 17/409 [>.............................] - ETA: 11s - loss: 0.8716 - dice_coefficient: 0.5751\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 20/409 [>.............................] - ETA: 11s - loss: 0.8340 - dice_coefficient: 0.5949\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 23/409 [>.............................] - ETA: 10s - loss: 0.8043 - dice_coefficient: 0.6084\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 26/409 [>.............................] - ETA: 10s - loss: 0.8132 - dice_coefficient: 0.6045\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 29/409 [=>............................] - ETA: 9s - loss: 0.8113 - dice_coefficient: 0.6046 \n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 32/409 [=>............................] - ETA: 9s - loss: 0.8138 - dice_coefficient: 0.6015\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 35/409 [=>............................] - ETA: 9s - loss: 0.8087 - dice_coefficient: 0.6023\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 38/409 [=>............................] - ETA: 8s - loss: 0.7857 - dice_coefficient: 0.6150\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 41/409 [==>...........................] - ETA: 8s - loss: 0.8082 - dice_coefficient: 0.6039\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 44/409 [==>...........................] - ETA: 8s - loss: 0.8059 - dice_coefficient: 0.6023\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 47/409 [==>...........................] - ETA: 8s - loss: 0.7963 - dice_coefficient: 0.6050\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 50/409 [==>...........................] - ETA: 8s - loss: 0.8079 - dice_coefficient: 0.6024\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 53/409 [==>...........................] - ETA: 8s - loss: 0.8263 - dice_coefficient: 0.5938\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 56/409 [===>..........................] - ETA: 7s - loss: 0.8190 - dice_coefficient: 0.5960\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 59/409 [===>..........................] - ETA: 7s - loss: 0.8205 - dice_coefficient: 0.5991\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 62/409 [===>..........................] - ETA: 7s - loss: 0.8225 - dice_coefficient: 0.5975\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 65/409 [===>..........................] - ETA: 7s - loss: 0.8446 - dice_coefficient: 0.5890\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 68/409 [===>..........................] - ETA: 7s - loss: 0.8357 - dice_coefficient: 0.5904\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 71/409 [====>.........................] - ETA: 7s - loss: 0.8384 - dice_coefficient: 0.5895\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 74/409 [====>.........................] - ETA: 7s - loss: 0.8482 - dice_coefficient: 0.5837\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 77/409 [====>.........................] - ETA: 7s - loss: 0.8355 - dice_coefficient: 0.5892\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 80/409 [====>.........................] - ETA: 7s - loss: 0.8349 - dice_coefficient: 0.5889\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 83/409 [=====>........................] - ETA: 6s - loss: 0.8241 - dice_coefficient: 0.5942\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 86/409 [=====>........................] - ETA: 6s - loss: 0.8151 - dice_coefficient: 0.5963\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 89/409 [=====>........................] - ETA: 6s - loss: 0.8156 - dice_coefficient: 0.5948\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 92/409 [=====>........................] - ETA: 6s - loss: 0.8113 - dice_coefficient: 0.5961\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 95/409 [=====>........................] - ETA: 6s - loss: 0.8131 - dice_coefficient: 0.5961\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n"," 98/409 [======>.......................] - ETA: 6s - loss: 0.8112 - dice_coefficient: 0.5966\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","101/409 [======>.......................] - ETA: 6s - loss: 0.8167 - dice_coefficient: 0.5967\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","104/409 [======>.......................] - ETA: 6s - loss: 0.8113 - dice_coefficient: 0.5967\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","107/409 [======>.......................] - ETA: 6s - loss: 0.8205 - dice_coefficient: 0.5942\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","110/409 [=======>......................] - ETA: 6s - loss: 0.8208 - dice_coefficient: 0.5938\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","113/409 [=======>......................] - ETA: 6s - loss: 0.8209 - dice_coefficient: 0.5938\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","116/409 [=======>......................] - ETA: 6s - loss: 0.8152 - dice_coefficient: 0.5946\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","119/409 [=======>......................] - ETA: 6s - loss: 0.8202 - dice_coefficient: 0.5908\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","122/409 [=======>......................] - ETA: 5s - loss: 0.8145 - dice_coefficient: 0.5920\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","125/409 [========>.....................] - ETA: 5s - loss: 0.8199 - dice_coefficient: 0.5941\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","128/409 [========>.....................] - ETA: 5s - loss: 0.8138 - dice_coefficient: 0.5964\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","131/409 [========>.....................] - ETA: 5s - loss: 0.8140 - dice_coefficient: 0.5972\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","134/409 [========>.....................] - ETA: 5s - loss: 0.8128 - dice_coefficient: 0.5982\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","137/409 [=========>....................] - ETA: 5s - loss: 0.8134 - dice_coefficient: 0.5978\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","140/409 [=========>....................] - ETA: 5s - loss: 0.8201 - dice_coefficient: 0.5951\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","143/409 [=========>....................] - ETA: 5s - loss: 0.8189 - dice_coefficient: 0.5954\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","146/409 [=========>....................] - ETA: 5s - loss: 0.8176 - dice_coefficient: 0.5945\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","149/409 [=========>....................] - ETA: 5s - loss: 0.8203 - dice_coefficient: 0.5934\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","152/409 [==========>...................] - ETA: 5s - loss: 0.8298 - dice_coefficient: 0.5906\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","155/409 [==========>...................] - ETA: 5s - loss: 0.8231 - dice_coefficient: 0.5924\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","158/409 [==========>...................] - ETA: 5s - loss: 0.8178 - dice_coefficient: 0.5941\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","161/409 [==========>...................] - ETA: 5s - loss: 0.8191 - dice_coefficient: 0.5948\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","164/409 [===========>..................] - ETA: 4s - loss: 0.8185 - dice_coefficient: 0.5944\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","167/409 [===========>..................] - ETA: 4s - loss: 0.8203 - dice_coefficient: 0.5931\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","170/409 [===========>..................] - ETA: 4s - loss: 0.8194 - dice_coefficient: 0.5922\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","173/409 [===========>..................] - ETA: 4s - loss: 0.8212 - dice_coefficient: 0.5920\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","176/409 [===========>..................] - ETA: 4s - loss: 0.8211 - dice_coefficient: 0.5921\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","179/409 [============>.................] - ETA: 4s - loss: 0.8219 - dice_coefficient: 0.5939\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","182/409 [============>.................] - ETA: 4s - loss: 0.8221 - dice_coefficient: 0.5938\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","185/409 [============>.................] - ETA: 4s - loss: 0.8240 - dice_coefficient: 0.5928\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","188/409 [============>.................] - ETA: 4s - loss: 0.8250 - dice_coefficient: 0.5927\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","191/409 [=============>................] - ETA: 4s - loss: 0.8258 - dice_coefficient: 0.5923\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","194/409 [=============>................] - ETA: 4s - loss: 0.8326 - dice_coefficient: 0.5889\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","197/409 [=============>................] - ETA: 4s - loss: 0.8302 - dice_coefficient: 0.5898\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","200/409 [=============>................] - ETA: 4s - loss: 0.8309 - dice_coefficient: 0.5896\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","203/409 [=============>................] - ETA: 4s - loss: 0.8324 - dice_coefficient: 0.5885\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","206/409 [==============>...............] - ETA: 4s - loss: 0.8358 - dice_coefficient: 0.5875\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","209/409 [==============>...............] - ETA: 3s - loss: 0.8345 - dice_coefficient: 0.5891\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","212/409 [==============>...............] - ETA: 3s - loss: 0.8366 - dice_coefficient: 0.5882\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","215/409 [==============>...............] - ETA: 3s - loss: 0.8355 - dice_coefficient: 0.5884\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","218/409 [==============>...............] - ETA: 3s - loss: 0.8428 - dice_coefficient: 0.5858\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","221/409 [===============>..............] - ETA: 3s - loss: 0.8448 - dice_coefficient: 0.5839\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","224/409 [===============>..............] - ETA: 3s - loss: 0.8463 - dice_coefficient: 0.5839\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","227/409 [===============>..............] - ETA: 3s - loss: 0.8575 - dice_coefficient: 0.5820\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","230/409 [===============>..............] - ETA: 3s - loss: 0.8547 - dice_coefficient: 0.5830\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","233/409 [================>.............] - ETA: 3s - loss: 0.8558 - dice_coefficient: 0.5824\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","236/409 [================>.............] - ETA: 3s - loss: 0.8563 - dice_coefficient: 0.5821\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","239/409 [================>.............] - ETA: 3s - loss: 0.8545 - dice_coefficient: 0.5834\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","242/409 [================>.............] - ETA: 3s - loss: 0.8557 - dice_coefficient: 0.5825\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","245/409 [================>.............] - ETA: 3s - loss: 0.8546 - dice_coefficient: 0.5828\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","248/409 [=================>............] - ETA: 3s - loss: 0.8544 - dice_coefficient: 0.5828\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","251/409 [=================>............] - ETA: 3s - loss: 0.8551 - dice_coefficient: 0.5829\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","254/409 [=================>............] - ETA: 3s - loss: 0.8565 - dice_coefficient: 0.5819\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","257/409 [=================>............] - ETA: 3s - loss: 0.8617 - dice_coefficient: 0.5796\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","260/409 [==================>...........] - ETA: 2s - loss: 0.8608 - dice_coefficient: 0.5791\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","263/409 [==================>...........] - ETA: 2s - loss: 0.8593 - dice_coefficient: 0.5797\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","266/409 [==================>...........] - ETA: 2s - loss: 0.8566 - dice_coefficient: 0.5804\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","269/409 [==================>...........] - ETA: 2s - loss: 0.8554 - dice_coefficient: 0.5808\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","272/409 [==================>...........] - ETA: 2s - loss: 0.8509 - dice_coefficient: 0.5826\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","275/409 [===================>..........] - ETA: 2s - loss: 0.8506 - dice_coefficient: 0.5819\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","278/409 [===================>..........] - ETA: 2s - loss: 0.8481 - dice_coefficient: 0.5832\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","281/409 [===================>..........] - ETA: 2s - loss: 0.8465 - dice_coefficient: 0.5837\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","284/409 [===================>..........] - ETA: 2s - loss: 0.8467 - dice_coefficient: 0.5835\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","287/409 [====================>.........] - ETA: 2s - loss: 0.8477 - dice_coefficient: 0.5829\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","290/409 [====================>.........] - ETA: 2s - loss: 0.8470 - dice_coefficient: 0.5829\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","293/409 [====================>.........] - ETA: 2s - loss: 0.8445 - dice_coefficient: 0.5834\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","296/409 [====================>.........] - ETA: 2s - loss: 0.8438 - dice_coefficient: 0.5834\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","299/409 [====================>.........] - ETA: 2s - loss: 0.8420 - dice_coefficient: 0.5839\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","302/409 [=====================>........] - ETA: 2s - loss: 0.8442 - dice_coefficient: 0.5837\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","305/409 [=====================>........] - ETA: 2s - loss: 0.8421 - dice_coefficient: 0.5849\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","308/409 [=====================>........] - ETA: 1s - loss: 0.8433 - dice_coefficient: 0.5844\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","311/409 [=====================>........] - ETA: 1s - loss: 0.8422 - dice_coefficient: 0.5850\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","314/409 [======================>.......] - ETA: 1s - loss: 0.8462 - dice_coefficient: 0.5835\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","317/409 [======================>.......] - ETA: 1s - loss: 0.8462 - dice_coefficient: 0.5839\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","320/409 [======================>.......] - ETA: 1s - loss: 0.8451 - dice_coefficient: 0.5844\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","323/409 [======================>.......] - ETA: 1s - loss: 0.8452 - dice_coefficient: 0.5849\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","326/409 [======================>.......] - ETA: 1s - loss: 0.8925 - dice_coefficient: 0.5830\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","329/409 [=======================>......] - ETA: 1s - loss: 0.8933 - dice_coefficient: 0.5827\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","332/409 [=======================>......] - ETA: 1s - loss: 0.8926 - dice_coefficient: 0.5828\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","335/409 [=======================>......] - ETA: 1s - loss: 0.8912 - dice_coefficient: 0.5836\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","338/409 [=======================>......] - ETA: 1s - loss: 0.8956 - dice_coefficient: 0.5822\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","341/409 [========================>.....] - ETA: 1s - loss: 0.8937 - dice_coefficient: 0.5823\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","344/409 [========================>.....] - ETA: 1s - loss: 0.8909 - dice_coefficient: 0.5825\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","347/409 [========================>.....] - ETA: 1s - loss: 0.8901 - dice_coefficient: 0.5825\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","350/409 [========================>.....] - ETA: 1s - loss: 0.8866 - dice_coefficient: 0.5832\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","353/409 [========================>.....] - ETA: 1s - loss: 0.8875 - dice_coefficient: 0.5822\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","356/409 [=========================>....] - ETA: 1s - loss: 0.8851 - dice_coefficient: 0.5831\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","359/409 [=========================>....] - ETA: 0s - loss: 0.8880 - dice_coefficient: 0.5817\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","362/409 [=========================>....] - ETA: 0s - loss: 0.8887 - dice_coefficient: 0.5815\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","365/409 [=========================>....] - ETA: 0s - loss: 0.8874 - dice_coefficient: 0.5820\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","368/409 [=========================>....] - ETA: 0s - loss: 0.8848 - dice_coefficient: 0.5829\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","371/409 [==========================>...] - ETA: 0s - loss: 0.8828 - dice_coefficient: 0.5832\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","374/409 [==========================>...] - ETA: 0s - loss: 0.8819 - dice_coefficient: 0.5830\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","377/409 [==========================>...] - ETA: 0s - loss: 0.8829 - dice_coefficient: 0.5831\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","380/409 [==========================>...] - ETA: 0s - loss: 0.8824 - dice_coefficient: 0.5827\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","383/409 [===========================>..] - ETA: 0s - loss: 0.8835 - dice_coefficient: 0.5819\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","386/409 [===========================>..] - ETA: 0s - loss: 0.8841 - dice_coefficient: 0.5810\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","389/409 [===========================>..] - ETA: 0s - loss: 0.8815 - dice_coefficient: 0.5816\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","392/409 [===========================>..] - ETA: 0s - loss: 0.8807 - dice_coefficient: 0.5814\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","395/409 [===========================>..] - ETA: 0s - loss: 0.8797 - dice_coefficient: 0.5814\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","398/409 [============================>.] - ETA: 0s - loss: 0.8787 - dice_coefficient: 0.5814\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","401/409 [============================>.] - ETA: 0s - loss: 0.8760 - dice_coefficient: 0.5820\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","404/409 [============================>.] - ETA: 0s - loss: 0.8758 - dice_coefficient: 0.5826\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","407/409 [============================>.] - ETA: 0s - loss: 0.8771 - dice_coefficient: 0.5824\n","Epoch 00002: loss did not improve from 0.51936\n","\n","Epoch 00002: loss did not improve from 0.51936\n","409/409 [==============================] - 8s 20ms/step - loss: 0.8768 - dice_coefficient: 0.5825 - lr: 0.0010\n","Epoch 3/5\n","\n","Epoch 00003: loss did not improve from 0.51936\n","  1/409 [..............................] - ETA: 0s - loss: 0.6831 - dice_coefficient: 0.5969\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","  4/409 [..............................] - ETA: 5s - loss: 0.5979 - dice_coefficient: 0.7023\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","  7/409 [..............................] - ETA: 6s - loss: 0.5417 - dice_coefficient: 0.7127\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 10/409 [..............................] - ETA: 6s - loss: 0.6139 - dice_coefficient: 0.6788\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 13/409 [..............................] - ETA: 6s - loss: 0.6440 - dice_coefficient: 0.6505\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 16/409 [>.............................] - ETA: 6s - loss: 0.6990 - dice_coefficient: 0.6339\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 19/409 [>.............................] - ETA: 7s - loss: 0.7416 - dice_coefficient: 0.6168\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 22/409 [>.............................] - ETA: 7s - loss: 0.7386 - dice_coefficient: 0.6112\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 25/409 [>.............................] - ETA: 6s - loss: 0.7696 - dice_coefficient: 0.5972\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 28/409 [=>............................] - ETA: 6s - loss: 0.7474 - dice_coefficient: 0.6083\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 31/409 [=>............................] - ETA: 6s - loss: 0.7774 - dice_coefficient: 0.6040\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 34/409 [=>............................] - ETA: 6s - loss: 0.7796 - dice_coefficient: 0.5996\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 37/409 [=>............................] - ETA: 6s - loss: 0.7677 - dice_coefficient: 0.6039\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 40/409 [=>............................] - ETA: 6s - loss: 0.7829 - dice_coefficient: 0.6046\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 43/409 [==>...........................] - ETA: 6s - loss: 0.7787 - dice_coefficient: 0.6088\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 46/409 [==>...........................] - ETA: 6s - loss: 0.7855 - dice_coefficient: 0.6059\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 49/409 [==>...........................] - ETA: 6s - loss: 0.7933 - dice_coefficient: 0.6036\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 52/409 [==>...........................] - ETA: 6s - loss: 0.7885 - dice_coefficient: 0.6061\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 55/409 [===>..........................] - ETA: 6s - loss: 0.7686 - dice_coefficient: 0.6130\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 58/409 [===>..........................] - ETA: 6s - loss: 0.7605 - dice_coefficient: 0.6145\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 61/409 [===>..........................] - ETA: 6s - loss: 0.7502 - dice_coefficient: 0.6171\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 64/409 [===>..........................] - ETA: 6s - loss: 0.7677 - dice_coefficient: 0.6108\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 67/409 [===>..........................] - ETA: 6s - loss: 0.7589 - dice_coefficient: 0.6174\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 70/409 [====>.........................] - ETA: 6s - loss: 0.7584 - dice_coefficient: 0.6171\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 73/409 [====>.........................] - ETA: 6s - loss: 0.7849 - dice_coefficient: 0.6059\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 76/409 [====>.........................] - ETA: 6s - loss: 0.7702 - dice_coefficient: 0.6126\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 79/409 [====>.........................] - ETA: 6s - loss: 0.7737 - dice_coefficient: 0.6090\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 82/409 [=====>........................] - ETA: 6s - loss: 0.7650 - dice_coefficient: 0.6127\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 85/409 [=====>........................] - ETA: 6s - loss: 0.7654 - dice_coefficient: 0.6124\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 88/409 [=====>........................] - ETA: 6s - loss: 0.7780 - dice_coefficient: 0.6067\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 91/409 [=====>........................] - ETA: 5s - loss: 0.7771 - dice_coefficient: 0.6088\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 94/409 [=====>........................] - ETA: 5s - loss: 0.7763 - dice_coefficient: 0.6083\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n"," 97/409 [======>.......................] - ETA: 5s - loss: 0.7773 - dice_coefficient: 0.6083\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","100/409 [======>.......................] - ETA: 5s - loss: 0.7716 - dice_coefficient: 0.6088\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","103/409 [======>.......................] - ETA: 5s - loss: 0.7649 - dice_coefficient: 0.6128\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","106/409 [======>.......................] - ETA: 5s - loss: 0.7739 - dice_coefficient: 0.6080\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","109/409 [======>.......................] - ETA: 5s - loss: 0.7724 - dice_coefficient: 0.6090\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","112/409 [=======>......................] - ETA: 5s - loss: 0.7658 - dice_coefficient: 0.6100\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","115/409 [=======>......................] - ETA: 5s - loss: 0.7645 - dice_coefficient: 0.6104\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","118/409 [=======>......................] - ETA: 5s - loss: 0.7692 - dice_coefficient: 0.6107\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","121/409 [=======>......................] - ETA: 5s - loss: 0.7672 - dice_coefficient: 0.6111\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","124/409 [========>.....................] - ETA: 5s - loss: 0.7680 - dice_coefficient: 0.6096\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","127/409 [========>.....................] - ETA: 5s - loss: 0.7644 - dice_coefficient: 0.6103\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","130/409 [========>.....................] - ETA: 5s - loss: 0.7671 - dice_coefficient: 0.6087\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","133/409 [========>.....................] - ETA: 5s - loss: 0.7705 - dice_coefficient: 0.6069\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","136/409 [========>.....................] - ETA: 5s - loss: 0.7690 - dice_coefficient: 0.6086\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","139/409 [=========>....................] - ETA: 5s - loss: 0.7670 - dice_coefficient: 0.6091\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","142/409 [=========>....................] - ETA: 5s - loss: 0.7654 - dice_coefficient: 0.6109\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","145/409 [=========>....................] - ETA: 4s - loss: 0.7629 - dice_coefficient: 0.6109\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","148/409 [=========>....................] - ETA: 4s - loss: 0.7606 - dice_coefficient: 0.6117\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","151/409 [==========>...................] - ETA: 4s - loss: 0.7624 - dice_coefficient: 0.6120\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","154/409 [==========>...................] - ETA: 4s - loss: 0.7607 - dice_coefficient: 0.6113\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","157/409 [==========>...................] - ETA: 4s - loss: 0.7642 - dice_coefficient: 0.6104\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","160/409 [==========>...................] - ETA: 4s - loss: 0.7601 - dice_coefficient: 0.6114\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","163/409 [==========>...................] - ETA: 4s - loss: 0.7736 - dice_coefficient: 0.6081\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","166/409 [===========>..................] - ETA: 4s - loss: 0.7804 - dice_coefficient: 0.6048\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","169/409 [===========>..................] - ETA: 4s - loss: 0.7826 - dice_coefficient: 0.6039\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","172/409 [===========>..................] - ETA: 4s - loss: 0.7848 - dice_coefficient: 0.6027\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","175/409 [===========>..................] - ETA: 4s - loss: 0.7816 - dice_coefficient: 0.6038\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","178/409 [============>.................] - ETA: 4s - loss: 0.7798 - dice_coefficient: 0.6049\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","181/409 [============>.................] - ETA: 4s - loss: 0.7751 - dice_coefficient: 0.6072\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","184/409 [============>.................] - ETA: 4s - loss: 0.7744 - dice_coefficient: 0.6067\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","187/409 [============>.................] - ETA: 4s - loss: 0.7710 - dice_coefficient: 0.6080\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","190/409 [============>.................] - ETA: 4s - loss: 0.7711 - dice_coefficient: 0.6078\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","193/409 [=============>................] - ETA: 4s - loss: 0.7768 - dice_coefficient: 0.6057\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","196/409 [=============>................] - ETA: 4s - loss: 0.7762 - dice_coefficient: 0.6058\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","199/409 [=============>................] - ETA: 3s - loss: 0.7716 - dice_coefficient: 0.6074\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","202/409 [=============>................] - ETA: 3s - loss: 0.7741 - dice_coefficient: 0.6061\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","205/409 [==============>...............] - ETA: 3s - loss: 0.7732 - dice_coefficient: 0.6067\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","208/409 [==============>...............] - ETA: 3s - loss: 0.7688 - dice_coefficient: 0.6084\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","211/409 [==============>...............] - ETA: 3s - loss: 0.7667 - dice_coefficient: 0.6088\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","214/409 [==============>...............] - ETA: 3s - loss: 0.8378 - dice_coefficient: 0.6063\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","217/409 [==============>...............] - ETA: 3s - loss: 0.8345 - dice_coefficient: 0.6069\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","220/409 [===============>..............] - ETA: 3s - loss: 0.8344 - dice_coefficient: 0.6062\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","223/409 [===============>..............] - ETA: 3s - loss: 0.8320 - dice_coefficient: 0.6065\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","226/409 [===============>..............] - ETA: 3s - loss: 0.8314 - dice_coefficient: 0.6065\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","229/409 [===============>..............] - ETA: 3s - loss: 0.8296 - dice_coefficient: 0.6070\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","232/409 [================>.............] - ETA: 3s - loss: 0.8307 - dice_coefficient: 0.6060\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","235/409 [================>.............] - ETA: 3s - loss: 0.8295 - dice_coefficient: 0.6068\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","238/409 [================>.............] - ETA: 3s - loss: 0.8278 - dice_coefficient: 0.6064\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","241/409 [================>.............] - ETA: 3s - loss: 0.8257 - dice_coefficient: 0.6064\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","244/409 [================>.............] - ETA: 3s - loss: 0.8249 - dice_coefficient: 0.6059\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","247/409 [=================>............] - ETA: 3s - loss: 0.8231 - dice_coefficient: 0.6057\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","250/409 [=================>............] - ETA: 3s - loss: 0.8229 - dice_coefficient: 0.6048\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","253/409 [=================>............] - ETA: 2s - loss: 0.8234 - dice_coefficient: 0.6045\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","256/409 [=================>............] - ETA: 2s - loss: 0.8212 - dice_coefficient: 0.6052\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","259/409 [=================>............] - ETA: 2s - loss: 0.8234 - dice_coefficient: 0.6049\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","262/409 [==================>...........] - ETA: 2s - loss: 0.8257 - dice_coefficient: 0.6031\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","265/409 [==================>...........] - ETA: 2s - loss: 0.8274 - dice_coefficient: 0.6024\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","268/409 [==================>...........] - ETA: 2s - loss: 0.8261 - dice_coefficient: 0.6027\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","271/409 [==================>...........] - ETA: 2s - loss: 0.8233 - dice_coefficient: 0.6038\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","274/409 [===================>..........] - ETA: 2s - loss: 0.8245 - dice_coefficient: 0.6027\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","277/409 [===================>..........] - ETA: 2s - loss: 0.8221 - dice_coefficient: 0.6043\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","280/409 [===================>..........] - ETA: 2s - loss: 0.8185 - dice_coefficient: 0.6057\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","283/409 [===================>..........] - ETA: 2s - loss: 0.8198 - dice_coefficient: 0.6050\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","286/409 [===================>..........] - ETA: 2s - loss: 0.8204 - dice_coefficient: 0.6043\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","289/409 [====================>.........] - ETA: 2s - loss: 0.8195 - dice_coefficient: 0.6052\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","292/409 [====================>.........] - ETA: 2s - loss: 0.8167 - dice_coefficient: 0.6064\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","295/409 [====================>.........] - ETA: 2s - loss: 0.8185 - dice_coefficient: 0.6062\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","298/409 [====================>.........] - ETA: 2s - loss: 0.8171 - dice_coefficient: 0.6065\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","301/409 [=====================>........] - ETA: 2s - loss: 0.8183 - dice_coefficient: 0.6066\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","304/409 [=====================>........] - ETA: 1s - loss: 0.8174 - dice_coefficient: 0.6070\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","307/409 [=====================>........] - ETA: 1s - loss: 0.8156 - dice_coefficient: 0.6081\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","310/409 [=====================>........] - ETA: 1s - loss: 0.8192 - dice_coefficient: 0.6062\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","313/409 [=====================>........] - ETA: 1s - loss: 0.8211 - dice_coefficient: 0.6055\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","316/409 [======================>.......] - ETA: 1s - loss: 0.8210 - dice_coefficient: 0.6057\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","319/409 [======================>.......] - ETA: 1s - loss: 0.8195 - dice_coefficient: 0.6068\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","322/409 [======================>.......] - ETA: 1s - loss: 0.8193 - dice_coefficient: 0.6064\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","325/409 [======================>.......] - ETA: 1s - loss: 0.8190 - dice_coefficient: 0.6064\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","328/409 [=======================>......] - ETA: 1s - loss: 0.8242 - dice_coefficient: 0.6038\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","331/409 [=======================>......] - ETA: 1s - loss: 0.8231 - dice_coefficient: 0.6037\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","334/409 [=======================>......] - ETA: 1s - loss: 0.8243 - dice_coefficient: 0.6029\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","337/409 [=======================>......] - ETA: 1s - loss: 0.8278 - dice_coefficient: 0.6016\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","340/409 [=======================>......] - ETA: 1s - loss: 0.8287 - dice_coefficient: 0.6011\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","343/409 [========================>.....] - ETA: 1s - loss: 0.8276 - dice_coefficient: 0.6007\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","346/409 [========================>.....] - ETA: 1s - loss: 0.8261 - dice_coefficient: 0.6013\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","349/409 [========================>.....] - ETA: 1s - loss: 0.8250 - dice_coefficient: 0.6013\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","352/409 [========================>.....] - ETA: 1s - loss: 0.8276 - dice_coefficient: 0.6007\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","355/409 [=========================>....] - ETA: 1s - loss: 0.8254 - dice_coefficient: 0.6012\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","358/409 [=========================>....] - ETA: 0s - loss: 0.8248 - dice_coefficient: 0.6014\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","361/409 [=========================>....] - ETA: 0s - loss: 0.8262 - dice_coefficient: 0.6003\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","364/409 [=========================>....] - ETA: 0s - loss: 0.8247 - dice_coefficient: 0.6003\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","367/409 [=========================>....] - ETA: 0s - loss: 0.8253 - dice_coefficient: 0.5998\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","370/409 [==========================>...] - ETA: 0s - loss: 0.8249 - dice_coefficient: 0.5999\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","373/409 [==========================>...] - ETA: 0s - loss: 0.8227 - dice_coefficient: 0.6007\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","376/409 [==========================>...] - ETA: 0s - loss: 0.8264 - dice_coefficient: 0.6005\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","379/409 [==========================>...] - ETA: 0s - loss: 0.8253 - dice_coefficient: 0.6010\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","382/409 [===========================>..] - ETA: 0s - loss: 0.8272 - dice_coefficient: 0.6006\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","385/409 [===========================>..] - ETA: 0s - loss: 0.8282 - dice_coefficient: 0.5998\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","388/409 [===========================>..] - ETA: 0s - loss: 0.8284 - dice_coefficient: 0.5999\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","391/409 [===========================>..] - ETA: 0s - loss: 0.8268 - dice_coefficient: 0.6002\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","394/409 [===========================>..] - ETA: 0s - loss: 0.8277 - dice_coefficient: 0.5991\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","397/409 [============================>.] - ETA: 0s - loss: 0.8288 - dice_coefficient: 0.5992\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","400/409 [============================>.] - ETA: 0s - loss: 0.8296 - dice_coefficient: 0.5988\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","403/409 [============================>.] - ETA: 0s - loss: 0.8290 - dice_coefficient: 0.5991\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","406/409 [============================>.] - ETA: 0s - loss: 0.8296 - dice_coefficient: 0.5990\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","\n","Epoch 00003: loss did not improve from 0.51936\n","409/409 [==============================] - 8s 19ms/step - loss: 0.8271 - dice_coefficient: 0.6000 - lr: 0.0010\n","Epoch 4/5\n","\n","Epoch 00004: loss did not improve from 0.51936\n","  1/409 [..............................] - ETA: 0s - loss: 0.5255 - dice_coefficient: 0.6811\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","  4/409 [..............................] - ETA: 5s - loss: 0.9219 - dice_coefficient: 0.5247\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","  7/409 [..............................] - ETA: 6s - loss: 0.8858 - dice_coefficient: 0.5192\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 10/409 [..............................] - ETA: 6s - loss: 0.8148 - dice_coefficient: 0.5620\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 13/409 [..............................] - ETA: 6s - loss: 0.8136 - dice_coefficient: 0.5836\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 16/409 [>.............................] - ETA: 6s - loss: 0.8039 - dice_coefficient: 0.5727\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 19/409 [>.............................] - ETA: 7s - loss: 0.7832 - dice_coefficient: 0.5854\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 22/409 [>.............................] - ETA: 7s - loss: 0.7863 - dice_coefficient: 0.6048\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 25/409 [>.............................] - ETA: 6s - loss: 0.7870 - dice_coefficient: 0.6047\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 28/409 [=>............................] - ETA: 6s - loss: 0.7929 - dice_coefficient: 0.6072\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 31/409 [=>............................] - ETA: 6s - loss: 0.7743 - dice_coefficient: 0.6128\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 34/409 [=>............................] - ETA: 6s - loss: 0.7714 - dice_coefficient: 0.6136\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 37/409 [=>............................] - ETA: 6s - loss: 0.7611 - dice_coefficient: 0.6156\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 40/409 [=>............................] - ETA: 6s - loss: 0.7512 - dice_coefficient: 0.6211\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 43/409 [==>...........................] - ETA: 6s - loss: 0.7408 - dice_coefficient: 0.6209\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 46/409 [==>...........................] - ETA: 6s - loss: 0.7295 - dice_coefficient: 0.6250\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 49/409 [==>...........................] - ETA: 6s - loss: 0.7344 - dice_coefficient: 0.6202\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 52/409 [==>...........................] - ETA: 6s - loss: 0.7654 - dice_coefficient: 0.6094\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 55/409 [===>..........................] - ETA: 6s - loss: 0.7603 - dice_coefficient: 0.6135\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 58/409 [===>..........................] - ETA: 6s - loss: 0.7589 - dice_coefficient: 0.6138\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 61/409 [===>..........................] - ETA: 6s - loss: 0.7515 - dice_coefficient: 0.6167\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 64/409 [===>..........................] - ETA: 6s - loss: 0.7569 - dice_coefficient: 0.6188\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 67/409 [===>..........................] - ETA: 6s - loss: 0.7602 - dice_coefficient: 0.6177\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 70/409 [====>.........................] - ETA: 6s - loss: 0.7523 - dice_coefficient: 0.6191\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 73/409 [====>.........................] - ETA: 6s - loss: 0.7419 - dice_coefficient: 0.6229\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 76/409 [====>.........................] - ETA: 6s - loss: 0.7434 - dice_coefficient: 0.6211\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 79/409 [====>.........................] - ETA: 6s - loss: 0.7347 - dice_coefficient: 0.6250\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 82/409 [=====>........................] - ETA: 6s - loss: 0.7350 - dice_coefficient: 0.6242\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 85/409 [=====>........................] - ETA: 6s - loss: 0.7405 - dice_coefficient: 0.6214\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 88/409 [=====>........................] - ETA: 6s - loss: 0.7348 - dice_coefficient: 0.6257\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 91/409 [=====>........................] - ETA: 6s - loss: 0.7525 - dice_coefficient: 0.6179\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 94/409 [=====>........................] - ETA: 5s - loss: 0.7484 - dice_coefficient: 0.6200\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n"," 97/409 [======>.......................] - ETA: 5s - loss: 0.7469 - dice_coefficient: 0.6205\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","100/409 [======>.......................] - ETA: 5s - loss: 0.7445 - dice_coefficient: 0.6201\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","103/409 [======>.......................] - ETA: 5s - loss: 0.7435 - dice_coefficient: 0.6195\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","106/409 [======>.......................] - ETA: 5s - loss: 0.7498 - dice_coefficient: 0.6173\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","109/409 [======>.......................] - ETA: 5s - loss: 0.7470 - dice_coefficient: 0.6190\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","112/409 [=======>......................] - ETA: 5s - loss: 0.7429 - dice_coefficient: 0.6190\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","115/409 [=======>......................] - ETA: 5s - loss: 0.7388 - dice_coefficient: 0.6213\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","118/409 [=======>......................] - ETA: 5s - loss: 0.7406 - dice_coefficient: 0.6194\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","121/409 [=======>......................] - ETA: 5s - loss: 0.7391 - dice_coefficient: 0.6206\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","124/409 [========>.....................] - ETA: 5s - loss: 0.7362 - dice_coefficient: 0.6210\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","127/409 [========>.....................] - ETA: 5s - loss: 0.7430 - dice_coefficient: 0.6189\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","130/409 [========>.....................] - ETA: 5s - loss: 0.7398 - dice_coefficient: 0.6206\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","133/409 [========>.....................] - ETA: 5s - loss: 0.7409 - dice_coefficient: 0.6202\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","136/409 [========>.....................] - ETA: 5s - loss: 0.7409 - dice_coefficient: 0.6202\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","139/409 [=========>....................] - ETA: 5s - loss: 0.7462 - dice_coefficient: 0.6190\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","142/409 [=========>....................] - ETA: 5s - loss: 0.7454 - dice_coefficient: 0.6198\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","145/409 [=========>....................] - ETA: 5s - loss: 0.7439 - dice_coefficient: 0.6213\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","148/409 [=========>....................] - ETA: 4s - loss: 0.7516 - dice_coefficient: 0.6193\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","151/409 [==========>...................] - ETA: 4s - loss: 0.7517 - dice_coefficient: 0.6197\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","154/409 [==========>...................] - ETA: 4s - loss: 0.7500 - dice_coefficient: 0.6199\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","157/409 [==========>...................] - ETA: 4s - loss: 0.7522 - dice_coefficient: 0.6191\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","160/409 [==========>...................] - ETA: 4s - loss: 0.7534 - dice_coefficient: 0.6179\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","163/409 [==========>...................] - ETA: 4s - loss: 0.7562 - dice_coefficient: 0.6190\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","166/409 [===========>..................] - ETA: 4s - loss: 0.7581 - dice_coefficient: 0.6186\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","169/409 [===========>..................] - ETA: 4s - loss: 0.7559 - dice_coefficient: 0.6189\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","172/409 [===========>..................] - ETA: 4s - loss: 0.7544 - dice_coefficient: 0.6200\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","175/409 [===========>..................] - ETA: 4s - loss: 0.7533 - dice_coefficient: 0.6199\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","178/409 [============>.................] - ETA: 4s - loss: 0.7538 - dice_coefficient: 0.6192\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","181/409 [============>.................] - ETA: 4s - loss: 0.7510 - dice_coefficient: 0.6199\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","184/409 [============>.................] - ETA: 4s - loss: 0.7466 - dice_coefficient: 0.6223\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","187/409 [============>.................] - ETA: 4s - loss: 0.7506 - dice_coefficient: 0.6202\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","190/409 [============>.................] - ETA: 4s - loss: 0.7474 - dice_coefficient: 0.6208\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","193/409 [=============>................] - ETA: 4s - loss: 0.7430 - dice_coefficient: 0.6226\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","196/409 [=============>................] - ETA: 4s - loss: 0.7428 - dice_coefficient: 0.6222\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","199/409 [=============>................] - ETA: 3s - loss: 0.7430 - dice_coefficient: 0.6211\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","202/409 [=============>................] - ETA: 3s - loss: 0.7488 - dice_coefficient: 0.6211\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","205/409 [==============>...............] - ETA: 3s - loss: 0.8257 - dice_coefficient: 0.6167\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","208/409 [==============>...............] - ETA: 3s - loss: 0.8271 - dice_coefficient: 0.6151\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","211/409 [==============>...............] - ETA: 3s - loss: 0.8261 - dice_coefficient: 0.6155\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","214/409 [==============>...............] - ETA: 3s - loss: 0.8215 - dice_coefficient: 0.6173\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","217/409 [==============>...............] - ETA: 3s - loss: 0.8223 - dice_coefficient: 0.6156\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","220/409 [===============>..............] - ETA: 3s - loss: 0.8198 - dice_coefficient: 0.6157\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","223/409 [===============>..............] - ETA: 3s - loss: 0.8243 - dice_coefficient: 0.6134\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","226/409 [===============>..............] - ETA: 3s - loss: 0.8226 - dice_coefficient: 0.6134\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","229/409 [===============>..............] - ETA: 3s - loss: 0.8219 - dice_coefficient: 0.6141\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","232/409 [================>.............] - ETA: 3s - loss: 0.8229 - dice_coefficient: 0.6137\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","235/409 [================>.............] - ETA: 3s - loss: 0.8219 - dice_coefficient: 0.6138\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","238/409 [================>.............] - ETA: 3s - loss: 0.8182 - dice_coefficient: 0.6151\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","241/409 [================>.............] - ETA: 3s - loss: 0.8152 - dice_coefficient: 0.6161\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","244/409 [================>.............] - ETA: 3s - loss: 0.8135 - dice_coefficient: 0.6169\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","247/409 [=================>............] - ETA: 3s - loss: 0.8124 - dice_coefficient: 0.6164\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","250/409 [=================>............] - ETA: 3s - loss: 0.8102 - dice_coefficient: 0.6166\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","253/409 [=================>............] - ETA: 2s - loss: 0.8092 - dice_coefficient: 0.6168\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","256/409 [=================>............] - ETA: 2s - loss: 0.8138 - dice_coefficient: 0.6149\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","259/409 [=================>............] - ETA: 2s - loss: 0.8168 - dice_coefficient: 0.6148\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","262/409 [==================>...........] - ETA: 2s - loss: 0.8158 - dice_coefficient: 0.6140\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","265/409 [==================>...........] - ETA: 2s - loss: 0.8178 - dice_coefficient: 0.6135\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","268/409 [==================>...........] - ETA: 2s - loss: 0.8195 - dice_coefficient: 0.6122\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","271/409 [==================>...........] - ETA: 2s - loss: 0.8166 - dice_coefficient: 0.6132\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","274/409 [===================>..........] - ETA: 2s - loss: 0.8140 - dice_coefficient: 0.6141\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","277/409 [===================>..........] - ETA: 2s - loss: 0.8098 - dice_coefficient: 0.6161\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","280/409 [===================>..........] - ETA: 2s - loss: 0.8093 - dice_coefficient: 0.6168\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","283/409 [===================>..........] - ETA: 2s - loss: 0.8076 - dice_coefficient: 0.6172\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","286/409 [===================>..........] - ETA: 2s - loss: 0.8101 - dice_coefficient: 0.6160\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","289/409 [====================>.........] - ETA: 2s - loss: 0.8130 - dice_coefficient: 0.6143\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","292/409 [====================>.........] - ETA: 2s - loss: 0.8116 - dice_coefficient: 0.6141\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","295/409 [====================>.........] - ETA: 2s - loss: 0.8079 - dice_coefficient: 0.6154\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","298/409 [====================>.........] - ETA: 2s - loss: 0.8079 - dice_coefficient: 0.6143\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","301/409 [=====================>........] - ETA: 2s - loss: 0.8088 - dice_coefficient: 0.6139\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","304/409 [=====================>........] - ETA: 1s - loss: 0.8093 - dice_coefficient: 0.6130\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","307/409 [=====================>........] - ETA: 1s - loss: 0.8086 - dice_coefficient: 0.6133\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","310/409 [=====================>........] - ETA: 1s - loss: 0.8098 - dice_coefficient: 0.6138\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","313/409 [=====================>........] - ETA: 1s - loss: 0.8102 - dice_coefficient: 0.6140\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","316/409 [======================>.......] - ETA: 1s - loss: 0.8143 - dice_coefficient: 0.6124\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","319/409 [======================>.......] - ETA: 1s - loss: 0.8155 - dice_coefficient: 0.6121\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","322/409 [======================>.......] - ETA: 1s - loss: 0.8138 - dice_coefficient: 0.6130\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","325/409 [======================>.......] - ETA: 1s - loss: 0.8134 - dice_coefficient: 0.6129\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","328/409 [=======================>......] - ETA: 1s - loss: 0.8114 - dice_coefficient: 0.6137\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","331/409 [=======================>......] - ETA: 1s - loss: 0.8134 - dice_coefficient: 0.6133\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","334/409 [=======================>......] - ETA: 1s - loss: 0.8112 - dice_coefficient: 0.6139\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","337/409 [=======================>......] - ETA: 1s - loss: 0.8114 - dice_coefficient: 0.6131\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","340/409 [=======================>......] - ETA: 1s - loss: 0.8096 - dice_coefficient: 0.6136\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","343/409 [========================>.....] - ETA: 1s - loss: 0.8125 - dice_coefficient: 0.6117\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","346/409 [========================>.....] - ETA: 1s - loss: 0.8132 - dice_coefficient: 0.6111\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","349/409 [========================>.....] - ETA: 1s - loss: 0.8120 - dice_coefficient: 0.6110\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","352/409 [========================>.....] - ETA: 1s - loss: 0.8126 - dice_coefficient: 0.6105\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","355/409 [=========================>....] - ETA: 1s - loss: 0.8125 - dice_coefficient: 0.6097\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","358/409 [=========================>....] - ETA: 0s - loss: 0.8116 - dice_coefficient: 0.6101\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","361/409 [=========================>....] - ETA: 0s - loss: 0.8095 - dice_coefficient: 0.6109\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","364/409 [=========================>....] - ETA: 0s - loss: 0.8080 - dice_coefficient: 0.6113\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","367/409 [=========================>....] - ETA: 0s - loss: 0.8076 - dice_coefficient: 0.6112\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","370/409 [==========================>...] - ETA: 0s - loss: 0.8102 - dice_coefficient: 0.6105\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","373/409 [==========================>...] - ETA: 0s - loss: 0.8119 - dice_coefficient: 0.6098\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","376/409 [==========================>...] - ETA: 0s - loss: 0.8099 - dice_coefficient: 0.6101\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","379/409 [==========================>...] - ETA: 0s - loss: 0.8086 - dice_coefficient: 0.6101\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","382/409 [===========================>..] - ETA: 0s - loss: 0.8084 - dice_coefficient: 0.6097\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","385/409 [===========================>..] - ETA: 0s - loss: 0.8071 - dice_coefficient: 0.6098\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","388/409 [===========================>..] - ETA: 0s - loss: 0.8083 - dice_coefficient: 0.6091\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","391/409 [===========================>..] - ETA: 0s - loss: 0.8066 - dice_coefficient: 0.6097\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","394/409 [===========================>..] - ETA: 0s - loss: 0.8083 - dice_coefficient: 0.6088\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","397/409 [============================>.] - ETA: 0s - loss: 0.8088 - dice_coefficient: 0.6084\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","400/409 [============================>.] - ETA: 0s - loss: 0.8055 - dice_coefficient: 0.6099\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","403/409 [============================>.] - ETA: 0s - loss: 0.8054 - dice_coefficient: 0.6102\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","406/409 [============================>.] - ETA: 0s - loss: 0.8075 - dice_coefficient: 0.6092\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","\n","Epoch 00004: loss did not improve from 0.51936\n","409/409 [==============================] - 8s 19ms/step - loss: 0.8108 - dice_coefficient: 0.6077 - lr: 0.0010\n","Epoch 5/5\n","\n","Epoch 00005: loss improved from 0.51936 to 0.51306, saving model to model-0.51.h5\n","  1/409 [..............................] - ETA: 0s - loss: 0.5131 - dice_coefficient: 0.7702WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.185419). Check your callbacks.\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","  4/409 [..............................] - ETA: 6s - loss: 0.7071 - dice_coefficient: 0.6260\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","  7/409 [..............................] - ETA: 6s - loss: 0.6986 - dice_coefficient: 0.6176\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 10/409 [..............................] - ETA: 7s - loss: 0.6653 - dice_coefficient: 0.6390\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 13/409 [..............................] - ETA: 7s - loss: 0.6184 - dice_coefficient: 0.6615\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 16/409 [>.............................] - ETA: 7s - loss: 0.5909 - dice_coefficient: 0.6833\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 19/409 [>.............................] - ETA: 7s - loss: 0.6706 - dice_coefficient: 0.6608\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 22/409 [>.............................] - ETA: 7s - loss: 0.7228 - dice_coefficient: 0.6454\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 25/409 [>.............................] - ETA: 7s - loss: 0.7116 - dice_coefficient: 0.6492\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 28/409 [=>............................] - ETA: 7s - loss: 0.7228 - dice_coefficient: 0.6348\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 31/409 [=>............................] - ETA: 7s - loss: 0.7650 - dice_coefficient: 0.6272\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 34/409 [=>............................] - ETA: 6s - loss: 0.7735 - dice_coefficient: 0.6334\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 37/409 [=>............................] - ETA: 6s - loss: 0.7814 - dice_coefficient: 0.6263\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 40/409 [=>............................] - ETA: 6s - loss: 0.7894 - dice_coefficient: 0.6253\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 43/409 [==>...........................] - ETA: 6s - loss: 0.7754 - dice_coefficient: 0.6304\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 46/409 [==>...........................] - ETA: 6s - loss: 0.7616 - dice_coefficient: 0.6356\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 49/409 [==>...........................] - ETA: 6s - loss: 0.7847 - dice_coefficient: 0.6260\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 52/409 [==>...........................] - ETA: 6s - loss: 0.7724 - dice_coefficient: 0.6331\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 55/409 [===>..........................] - ETA: 6s - loss: 0.7675 - dice_coefficient: 0.6391\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 58/409 [===>..........................] - ETA: 6s - loss: 0.7630 - dice_coefficient: 0.6391\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 61/409 [===>..........................] - ETA: 6s - loss: 0.7567 - dice_coefficient: 0.6387\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 64/409 [===>..........................] - ETA: 6s - loss: 0.7600 - dice_coefficient: 0.6364\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 67/409 [===>..........................] - ETA: 6s - loss: 0.7570 - dice_coefficient: 0.6343\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 70/409 [====>.........................] - ETA: 6s - loss: 0.7474 - dice_coefficient: 0.6373\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 73/409 [====>.........................] - ETA: 6s - loss: 0.7514 - dice_coefficient: 0.6343\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 76/409 [====>.........................] - ETA: 6s - loss: 0.7521 - dice_coefficient: 0.6325\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 79/409 [====>.........................] - ETA: 6s - loss: 0.7470 - dice_coefficient: 0.6323\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 82/409 [=====>........................] - ETA: 6s - loss: 0.7523 - dice_coefficient: 0.6332\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 85/409 [=====>........................] - ETA: 6s - loss: 0.7427 - dice_coefficient: 0.6374\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 88/409 [=====>........................] - ETA: 6s - loss: 0.7421 - dice_coefficient: 0.6365\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 91/409 [=====>........................] - ETA: 6s - loss: 0.7359 - dice_coefficient: 0.6378\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 94/409 [=====>........................] - ETA: 5s - loss: 0.7382 - dice_coefficient: 0.6362\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n"," 97/409 [======>.......................] - ETA: 5s - loss: 0.7295 - dice_coefficient: 0.6399\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","100/409 [======>.......................] - ETA: 5s - loss: 0.7247 - dice_coefficient: 0.6415\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","103/409 [======>.......................] - ETA: 5s - loss: 0.7281 - dice_coefficient: 0.6394\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","106/409 [======>.......................] - ETA: 5s - loss: 0.7296 - dice_coefficient: 0.6388\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","109/409 [======>.......................] - ETA: 5s - loss: 0.7263 - dice_coefficient: 0.6388\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","112/409 [=======>......................] - ETA: 5s - loss: 0.7250 - dice_coefficient: 0.6382\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","115/409 [=======>......................] - ETA: 5s - loss: 0.7213 - dice_coefficient: 0.6385\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","118/409 [=======>......................] - ETA: 5s - loss: 0.7170 - dice_coefficient: 0.6400\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","121/409 [=======>......................] - ETA: 5s - loss: 0.7207 - dice_coefficient: 0.6381\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","124/409 [========>.....................] - ETA: 5s - loss: 0.7135 - dice_coefficient: 0.6412\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","127/409 [========>.....................] - ETA: 5s - loss: 0.7141 - dice_coefficient: 0.6403\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","130/409 [========>.....................] - ETA: 5s - loss: 0.7113 - dice_coefficient: 0.6410\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","133/409 [========>.....................] - ETA: 5s - loss: 0.7164 - dice_coefficient: 0.6386\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","136/409 [========>.....................] - ETA: 5s - loss: 0.7138 - dice_coefficient: 0.6387\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","139/409 [=========>....................] - ETA: 5s - loss: 0.7094 - dice_coefficient: 0.6394\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","142/409 [=========>....................] - ETA: 5s - loss: 0.7100 - dice_coefficient: 0.6393\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","145/409 [=========>....................] - ETA: 4s - loss: 0.7140 - dice_coefficient: 0.6375\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","148/409 [=========>....................] - ETA: 4s - loss: 0.7097 - dice_coefficient: 0.6386\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","151/409 [==========>...................] - ETA: 4s - loss: 0.7135 - dice_coefficient: 0.6359\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","154/409 [==========>...................] - ETA: 4s - loss: 0.7154 - dice_coefficient: 0.6341\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","157/409 [==========>...................] - ETA: 4s - loss: 0.7233 - dice_coefficient: 0.6323\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","160/409 [==========>...................] - ETA: 4s - loss: 0.7237 - dice_coefficient: 0.6316\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","163/409 [==========>...................] - ETA: 4s - loss: 0.7235 - dice_coefficient: 0.6308\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","166/409 [===========>..................] - ETA: 4s - loss: 0.7290 - dice_coefficient: 0.6277\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","169/409 [===========>..................] - ETA: 4s - loss: 0.7297 - dice_coefficient: 0.6263\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","172/409 [===========>..................] - ETA: 4s - loss: 0.7303 - dice_coefficient: 0.6262\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","175/409 [===========>..................] - ETA: 4s - loss: 0.7275 - dice_coefficient: 0.6276\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","178/409 [============>.................] - ETA: 4s - loss: 0.7306 - dice_coefficient: 0.6264\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","181/409 [============>.................] - ETA: 4s - loss: 0.7278 - dice_coefficient: 0.6282\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","184/409 [============>.................] - ETA: 4s - loss: 0.7306 - dice_coefficient: 0.6276\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","187/409 [============>.................] - ETA: 4s - loss: 0.7286 - dice_coefficient: 0.6282\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","190/409 [============>.................] - ETA: 4s - loss: 0.7333 - dice_coefficient: 0.6272\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","193/409 [=============>................] - ETA: 4s - loss: 0.7329 - dice_coefficient: 0.6264\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","196/409 [=============>................] - ETA: 4s - loss: 0.7309 - dice_coefficient: 0.6268\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","199/409 [=============>................] - ETA: 3s - loss: 0.7301 - dice_coefficient: 0.6276\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","202/409 [=============>................] - ETA: 3s - loss: 0.7294 - dice_coefficient: 0.6281\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","205/409 [==============>...............] - ETA: 3s - loss: 0.7281 - dice_coefficient: 0.6295\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","208/409 [==============>...............] - ETA: 3s - loss: 0.7269 - dice_coefficient: 0.6293\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","211/409 [==============>...............] - ETA: 3s - loss: 0.8005 - dice_coefficient: 0.6256\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","214/409 [==============>...............] - ETA: 3s - loss: 0.7977 - dice_coefficient: 0.6263\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","217/409 [==============>...............] - ETA: 3s - loss: 0.7928 - dice_coefficient: 0.6283\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","220/409 [===============>..............] - ETA: 3s - loss: 0.7927 - dice_coefficient: 0.6283\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","223/409 [===============>..............] - ETA: 3s - loss: 0.7896 - dice_coefficient: 0.6288\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","226/409 [===============>..............] - ETA: 3s - loss: 0.7922 - dice_coefficient: 0.6276\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","229/409 [===============>..............] - ETA: 3s - loss: 0.7947 - dice_coefficient: 0.6276\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","232/409 [================>.............] - ETA: 3s - loss: 0.7918 - dice_coefficient: 0.6281\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","235/409 [================>.............] - ETA: 3s - loss: 0.7908 - dice_coefficient: 0.6281\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","238/409 [================>.............] - ETA: 3s - loss: 0.7878 - dice_coefficient: 0.6297\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","241/409 [================>.............] - ETA: 3s - loss: 0.7874 - dice_coefficient: 0.6291\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","244/409 [================>.............] - ETA: 3s - loss: 0.7891 - dice_coefficient: 0.6283\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","247/409 [=================>............] - ETA: 3s - loss: 0.7861 - dice_coefficient: 0.6288\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","250/409 [=================>............] - ETA: 3s - loss: 0.7920 - dice_coefficient: 0.6260\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","253/409 [=================>............] - ETA: 2s - loss: 0.7923 - dice_coefficient: 0.6249\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","256/409 [=================>............] - ETA: 2s - loss: 0.7920 - dice_coefficient: 0.6237\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","259/409 [=================>............] - ETA: 2s - loss: 0.7905 - dice_coefficient: 0.6235\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","262/409 [==================>...........] - ETA: 2s - loss: 0.7874 - dice_coefficient: 0.6243\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","265/409 [==================>...........] - ETA: 2s - loss: 0.7883 - dice_coefficient: 0.6231\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","268/409 [==================>...........] - ETA: 2s - loss: 0.7872 - dice_coefficient: 0.6231\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","271/409 [==================>...........] - ETA: 2s - loss: 0.7904 - dice_coefficient: 0.6215\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","274/409 [===================>..........] - ETA: 2s - loss: 0.7896 - dice_coefficient: 0.6223\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","277/409 [===================>..........] - ETA: 2s - loss: 0.7900 - dice_coefficient: 0.6210\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","280/409 [===================>..........] - ETA: 2s - loss: 0.7896 - dice_coefficient: 0.6208\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","283/409 [===================>..........] - ETA: 2s - loss: 0.7923 - dice_coefficient: 0.6199\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","286/409 [===================>..........] - ETA: 2s - loss: 0.7917 - dice_coefficient: 0.6200\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","289/409 [====================>.........] - ETA: 2s - loss: 0.7916 - dice_coefficient: 0.6193\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","292/409 [====================>.........] - ETA: 2s - loss: 0.7895 - dice_coefficient: 0.6197\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","295/409 [====================>.........] - ETA: 2s - loss: 0.7890 - dice_coefficient: 0.6191\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","298/409 [====================>.........] - ETA: 2s - loss: 0.7869 - dice_coefficient: 0.6203\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","301/409 [=====================>........] - ETA: 2s - loss: 0.7856 - dice_coefficient: 0.6204\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","304/409 [=====================>........] - ETA: 1s - loss: 0.7858 - dice_coefficient: 0.6213\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","307/409 [=====================>........] - ETA: 1s - loss: 0.7851 - dice_coefficient: 0.6214\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","310/409 [=====================>........] - ETA: 1s - loss: 0.7839 - dice_coefficient: 0.6220\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","313/409 [=====================>........] - ETA: 1s - loss: 0.7881 - dice_coefficient: 0.6204\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","316/409 [======================>.......] - ETA: 1s - loss: 0.7936 - dice_coefficient: 0.6187\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","319/409 [======================>.......] - ETA: 1s - loss: 0.7938 - dice_coefficient: 0.6186\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","322/409 [======================>.......] - ETA: 1s - loss: 0.7941 - dice_coefficient: 0.6187\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","325/409 [======================>.......] - ETA: 1s - loss: 0.7942 - dice_coefficient: 0.6193\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","328/409 [=======================>......] - ETA: 1s - loss: 0.7936 - dice_coefficient: 0.6192\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","331/409 [=======================>......] - ETA: 1s - loss: 0.7920 - dice_coefficient: 0.6192\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","334/409 [=======================>......] - ETA: 1s - loss: 0.7924 - dice_coefficient: 0.6190\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","337/409 [=======================>......] - ETA: 1s - loss: 0.7915 - dice_coefficient: 0.6197\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","340/409 [=======================>......] - ETA: 1s - loss: 0.7897 - dice_coefficient: 0.6197\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","343/409 [========================>.....] - ETA: 1s - loss: 0.7906 - dice_coefficient: 0.6192\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","346/409 [========================>.....] - ETA: 1s - loss: 0.7911 - dice_coefficient: 0.6190\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","349/409 [========================>.....] - ETA: 1s - loss: 0.7935 - dice_coefficient: 0.6185\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","352/409 [========================>.....] - ETA: 1s - loss: 0.7939 - dice_coefficient: 0.6180\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","355/409 [=========================>....] - ETA: 1s - loss: 0.7966 - dice_coefficient: 0.6169\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","358/409 [=========================>....] - ETA: 0s - loss: 0.7959 - dice_coefficient: 0.6174\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","361/409 [=========================>....] - ETA: 0s - loss: 0.7958 - dice_coefficient: 0.6169\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","364/409 [=========================>....] - ETA: 0s - loss: 0.7950 - dice_coefficient: 0.6169\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","367/409 [=========================>....] - ETA: 0s - loss: 0.7965 - dice_coefficient: 0.6158\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","370/409 [==========================>...] - ETA: 0s - loss: 0.7982 - dice_coefficient: 0.6146\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","373/409 [==========================>...] - ETA: 0s - loss: 0.7981 - dice_coefficient: 0.6152\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","376/409 [==========================>...] - ETA: 0s - loss: 0.8003 - dice_coefficient: 0.6138\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","379/409 [==========================>...] - ETA: 0s - loss: 0.7987 - dice_coefficient: 0.6148\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","382/409 [===========================>..] - ETA: 0s - loss: 0.7975 - dice_coefficient: 0.6150\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","385/409 [===========================>..] - ETA: 0s - loss: 0.7953 - dice_coefficient: 0.6159\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","388/409 [===========================>..] - ETA: 0s - loss: 0.7947 - dice_coefficient: 0.6153\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","391/409 [===========================>..] - ETA: 0s - loss: 0.7930 - dice_coefficient: 0.6156\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","394/409 [===========================>..] - ETA: 0s - loss: 0.7928 - dice_coefficient: 0.6164\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","397/409 [============================>.] - ETA: 0s - loss: 0.7918 - dice_coefficient: 0.6164\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","400/409 [============================>.] - ETA: 0s - loss: 0.7904 - dice_coefficient: 0.6165\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","403/409 [============================>.] - ETA: 0s - loss: 0.7901 - dice_coefficient: 0.6163\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","406/409 [============================>.] - ETA: 0s - loss: 0.7903 - dice_coefficient: 0.6157\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","\n","Epoch 00005: loss did not improve from 0.51306\n","409/409 [==============================] - 8s 19ms/step - loss: 0.7895 - dice_coefficient: 0.6159 - lr: 0.0010\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fd9ac694c88>"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5VtnuzlOf4uL"},"source":["### Get the predicted mask for a sample image   (5 marks)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"o-CBCMysrchu","colab":{}},"source":["n = 10\n","sample_image = X_train[n]\n","\n","WEIGHTS_FILE = '/content/drive/My Drive/Colab Notebooks/Computer Vision/model-0.51.h5'\n","model.load_weights(WEIGHTS_FILE)\n","\n","pred_mask = cv2.resize(1.0*(model.predict(x=np.array([sample_image]))[0] > 0.5), (IMAGE_WIDTH,IMAGE_HEIGHT))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fDIetz0HgA4R"},"source":["### Impose the mask on the image (5 marks)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MTAHGkb5xdzu","colab":{"base_uri":"https://localhost:8080/","height":303},"outputId":"b98a7ab7-019b-4632-8e3d-b8d39a5eb3c1","executionInfo":{"status":"ok","timestamp":1589951454091,"user_tz":-330,"elapsed":1422,"user":{"displayName":"Atul Kumar","photoUrl":"","userId":"02267315536838712153"}}},"source":["import matplotlib.pyplot as plt\n","\n","image2 = sample_image\n","image2[:,:,0] = pred_mask*sample_image[:,:,0]\n","image2[:,:,1] = pred_mask*sample_image[:,:,1]\n","image2[:,:,2] = pred_mask*sample_image[:,:,2]\n","\n","out_image = image2\n","\n","plt.imshow(out_image)"],"execution_count":62,"outputs":[{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fd9ab0c09e8>"]},"metadata":{"tags":[]},"execution_count":62},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}